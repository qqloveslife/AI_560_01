{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8328e9e1-619c-48ba-b5d9-39795cf0e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NOTEBOOK 3: TEST SCENARIO GENERATION\n",
      "============================================================\n",
      "\n",
      "✓ Core libraries imported\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NOTEBOOK 3: TEST SCENARIO GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Critical imports at the top\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\n✓ Core libraries imported\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "477b91f4-610d-4ab9-92fa-0052749c4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING PERSONAS AND PROTOTYPE DATA\n",
      "============================================================\n",
      "✓ Loaded 30 personas\n",
      "✓ Loaded prototype with 1 screens\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING PERSONAS AND PROTOTYPE DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load personas from Notebook 1\n",
    "personas_file = Path(\"./personas_output/generated_personas_30.json\")\n",
    "\n",
    "if personas_file.exists():\n",
    "    with open(personas_file, 'r') as f:\n",
    "        personas = json.load(f)\n",
    "    print(f\"✓ Loaded {len(personas)} personas\")\n",
    "else:\n",
    "    print(\"❌ Personas file not found!\")\n",
    "    print(\"Please run Notebook 1 first to generate personas\")\n",
    "    personas = []\n",
    "\n",
    "# Load prototype from Notebook 2\n",
    "prototype_file = Path(\"prototype_data.json\")\n",
    "\n",
    "if prototype_file.exists():\n",
    "    with open(prototype_file, 'r') as f:\n",
    "        prototype_data = json.load(f)\n",
    "    print(f\"✓ Loaded prototype with {prototype_data.get('screen_count', 0)} screens\")\n",
    "else:\n",
    "    print(\"❌ Prototype file not found!\")\n",
    "    print(\"Please run Notebook 2 first to parse prototype\")\n",
    "    prototype_data = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263e898f-c109-4cd6-bc11-74ba49bf1de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SCENARIO GENERATION FUNCTIONS\n",
      "============================================================\n",
      "✓ Scenario generation functions loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SCENARIO GENERATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_test_scenarios_for_persona(persona, prototype, model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Generate specific test scenarios for a persona\n",
    "    \n",
    "    Args:\n",
    "        persona: Persona dictionary\n",
    "        prototype: Prototype data\n",
    "        model: Language model (optional)\n",
    "        tokenizer: Tokenizer (optional)\n",
    "    \n",
    "    Returns:\n",
    "        List of test scenarios\n",
    "    \"\"\"\n",
    "    # Use global model if available\n",
    "    if model is None:\n",
    "        model = globals().get('model')\n",
    "    if tokenizer is None:\n",
    "        tokenizer = globals().get('tokenizer')\n",
    "    \n",
    "    scenarios = []\n",
    "    \n",
    "    # Extract persona characteristics\n",
    "    user_type = persona.get('user_type', 'User')\n",
    "    tech_level = persona.get('tech_proficiency', 'Intermediate')\n",
    "    goal = persona.get('primary_goal', 'Complete task')\n",
    "    pain_point = persona.get('pain_point', 'General usability')\n",
    "    \n",
    "    # Define scenario templates based on user characteristics\n",
    "    scenario_templates = [\n",
    "        {\n",
    "            \"task\": f\"Navigate to main feature and {goal.lower()}\",\n",
    "            \"expected_behavior\": f\"{user_type} with {tech_level} tech skills successfully completes primary workflow\",\n",
    "            \"success_criteria\": [\"Task completed within reasonable time\", \"No critical errors encountered\"],\n",
    "            \"focus_area\": \"Primary workflow completion\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": f\"Recover from error state related to: {pain_point}\",\n",
    "            \"expected_behavior\": f\"User encounters and resolves {pain_point} without abandoning task\",\n",
    "            \"success_criteria\": [\"Error message is clear\", \"Recovery path is obvious\"],\n",
    "            \"focus_area\": \"Error handling\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Find and use help/information resources\",\n",
    "            \"expected_behavior\": f\"{tech_level} user can locate assistance when needed\",\n",
    "            \"success_criteria\": [\"Help is discoverable\", \"Information is understandable\"],\n",
    "            \"focus_area\": \"Discoverability\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": f\"Complete secondary task: {goal}\",\n",
    "            \"expected_behavior\": \"User navigates to and completes secondary feature\",\n",
    "            \"success_criteria\": [\"Navigation is intuitive\", \"Task flow is logical\"],\n",
    "            \"focus_area\": \"Secondary workflows\"\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"Use interface on first visit (no prior knowledge)\",\n",
    "            \"expected_behavior\": f\"{user_type} understands interface without training\",\n",
    "            \"success_criteria\": [\"Purpose is clear\", \"Next steps are obvious\"],\n",
    "            \"focus_area\": \"Learnability\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create scenarios for this persona\n",
    "    for i, template in enumerate(scenario_templates):\n",
    "        scenario = {\n",
    "            \"scenario_id\": f\"{persona['id']}_scenario_{i+1}\",\n",
    "            \"persona_id\": persona['id'],\n",
    "            \"task_description\": template[\"task\"],\n",
    "            \"expected_behavior\": template[\"expected_behavior\"],\n",
    "            \"success_criteria\": template[\"success_criteria\"],\n",
    "            \"focus_area\": template[\"focus_area\"],\n",
    "            \"user_context\": {\n",
    "                \"user_type\": user_type,\n",
    "                \"tech_proficiency\": tech_level,\n",
    "                \"primary_goal\": goal,\n",
    "                \"pain_point\": pain_point\n",
    "            },\n",
    "            \"priority\": \"medium\",\n",
    "            \"estimated_duration\": \"5-10 minutes\"\n",
    "        }\n",
    "        \n",
    "        scenarios.append(scenario)\n",
    "    \n",
    "    # Try to enhance with LLM if available\n",
    "    if model and tokenizer:\n",
    "        try:\n",
    "            # Add LLM-generated context for first scenario\n",
    "            prompt = f\"\"\"For a {user_type} with {tech_level} tech proficiency who wants to {goal}, \n",
    "what specific usability issues might they encounter with {pain_point}?\n",
    "Provide 2-3 specific concerns in one sentence each.\"\"\"\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=256,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Add attention mask\n",
    "            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    attention_mask=inputs['attention_mask']\n",
    "                )\n",
    "            \n",
    "            enhanced_context = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if prompt in enhanced_context:\n",
    "                enhanced_context = enhanced_context.split(prompt)[-1].strip()\n",
    "            \n",
    "            scenarios[0]['llm_enhanced_context'] = enhanced_context[:200]\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Silently continue without LLM enhancement\n",
    "            pass\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "\n",
    "def prioritize_scenarios(scenarios):\n",
    "    \"\"\"\n",
    "    Prioritize scenarios based on user characteristics and risk\n",
    "    \n",
    "    Args:\n",
    "        scenarios: List of scenario dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Scenarios with priority levels assigned\n",
    "    \"\"\"\n",
    "    for scenario in scenarios:\n",
    "        score = 0\n",
    "        \n",
    "        # Get user context\n",
    "        context = scenario.get('user_context', {})\n",
    "        tech_level = context.get('tech_proficiency', 'Intermediate').lower()\n",
    "        focus = scenario.get('focus_area', '').lower()\n",
    "        \n",
    "        # High priority factors\n",
    "        if tech_level in ['beginner', 'limited']:\n",
    "            score += 3  # Beginners are high priority\n",
    "        \n",
    "        if focus in ['error handling', 'learnability']:\n",
    "            score += 2  # Critical areas\n",
    "        \n",
    "        if 'accessibility' in str(scenario).lower():\n",
    "            score += 2  # Accessibility is important\n",
    "        \n",
    "        if focus in ['primary workflow', 'main feature']:\n",
    "            score += 2  # Core functionality\n",
    "        \n",
    "        # Medium priority factors\n",
    "        if tech_level in ['intermediate', 'moderate']:\n",
    "            score += 1\n",
    "        \n",
    "        if focus in ['secondary workflows', 'navigation']:\n",
    "            score += 1\n",
    "        \n",
    "        # Assign priority based on score\n",
    "        if score >= 5:\n",
    "            scenario['priority'] = 'high'\n",
    "        elif score >= 3:\n",
    "            scenario['priority'] = 'medium'\n",
    "        else:\n",
    "            scenario['priority'] = 'low'\n",
    "    \n",
    "    return scenarios\n",
    "\n",
    "\n",
    "def create_test_execution_plan(scenarios, personas):\n",
    "    \"\"\"\n",
    "    Create an organized test execution plan\n",
    "    \n",
    "    Args:\n",
    "        scenarios: List of scenarios\n",
    "        personas: List of personas\n",
    "    \n",
    "    Returns:\n",
    "        Test execution plan dictionary\n",
    "    \"\"\"\n",
    "    # Group scenarios by priority\n",
    "    high_priority = [s for s in scenarios if s['priority'] == 'high']\n",
    "    medium_priority = [s for s in scenarios if s['priority'] == 'medium']\n",
    "    low_priority = [s for s in scenarios if s['priority'] == 'low']\n",
    "    \n",
    "    # Calculate estimates\n",
    "    total_scenarios = len(scenarios)\n",
    "    avg_duration_minutes = 7.5  # Average of 5-10 minutes\n",
    "    total_minutes = total_scenarios * avg_duration_minutes\n",
    "    \n",
    "    test_plan = {\n",
    "        \"plan_id\": f\"test_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"total_scenarios\": total_scenarios,\n",
    "        \"total_personas\": len(personas),\n",
    "        \"scenarios_by_priority\": {\n",
    "            \"high\": len(high_priority),\n",
    "            \"medium\": len(medium_priority),\n",
    "            \"low\": len(low_priority)\n",
    "        },\n",
    "        \"estimated_duration\": {\n",
    "            \"minutes\": int(total_minutes),\n",
    "            \"hours\": round(total_minutes / 60, 1)\n",
    "        },\n",
    "        \"execution_order\": {\n",
    "            \"phase_1_high_priority\": high_priority,\n",
    "            \"phase_2_medium_priority\": medium_priority,\n",
    "            \"phase_3_low_priority\": low_priority\n",
    "        },\n",
    "        \"all_scenarios\": scenarios\n",
    "    }\n",
    "    \n",
    "    return test_plan\n",
    "\n",
    "\n",
    "print(\"✓ Scenario generation functions loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df3e7dd-2eae-4144-ae3a-f22a321ce4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING TEST SCENARIOS\n",
      "============================================================\n",
      "Generating scenarios for 30 personas...\n",
      "This may take 2-3 minutes...\n",
      "\n",
      "Progress: 5/30 personas processed (25 scenarios generated)\n",
      "Progress: 10/30 personas processed (50 scenarios generated)\n",
      "Progress: 15/30 personas processed (75 scenarios generated)\n",
      "Progress: 20/30 personas processed (100 scenarios generated)\n",
      "Progress: 25/30 personas processed (125 scenarios generated)\n",
      "Progress: 30/30 personas processed (150 scenarios generated)\n",
      "\n",
      "✓ Generated 150 total test scenarios\n",
      "✓ Average: 5.0 scenarios per persona\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING TEST SCENARIOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not personas:\n",
    "    print(\"❌ No personas loaded. Cannot generate scenarios.\")\n",
    "    print(\"Please run Notebook 1 first.\")\n",
    "else:\n",
    "    print(f\"Generating scenarios for {len(personas)} personas...\")\n",
    "    print(\"This may take 2-3 minutes...\\n\")\n",
    "    \n",
    "    all_scenarios = []\n",
    "    \n",
    "    # Generate scenarios for each persona\n",
    "    for i, persona in enumerate(personas):\n",
    "        persona_scenarios = generate_test_scenarios_for_persona(\n",
    "            persona, \n",
    "            prototype_data,\n",
    "            model=globals().get('model'),\n",
    "            tokenizer=globals().get('tokenizer')\n",
    "        )\n",
    "        \n",
    "        all_scenarios.extend(persona_scenarios)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 5 == 0 or (i + 1) == len(personas):\n",
    "            print(f\"Progress: {i+1}/{len(personas)} personas processed ({len(all_scenarios)} scenarios generated)\")\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(all_scenarios)} total test scenarios\")\n",
    "    print(f\"✓ Average: {len(all_scenarios) / len(personas):.1f} scenarios per persona\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4290be-b657-4029-b213-a0e4e080bb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PRIORITIZING TEST SCENARIOS\n",
      "============================================================\n",
      "Analyzing and prioritizing scenarios...\n",
      "\n",
      "✓ Scenarios prioritized\n",
      "\n",
      "Priority Distribution:\n",
      "  • High Priority: 20 scenarios\n",
      "  • Medium Priority: 47 scenarios\n",
      "  • Low Priority: 83 scenarios\n",
      "\n",
      "------------------------------------------------------------\n",
      "TOP 3 HIGH PRIORITY SCENARIOS:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. persona_008_scenario_2\n",
      "   Task: Recover from error state related to: Limited customization options\n",
      "   Focus: Error handling\n",
      "   User: Small Business Owner (Limited)\n",
      "\n",
      "2. persona_008_scenario_5\n",
      "   Task: Use interface on first visit (no prior knowledge)\n",
      "   Focus: Learnability\n",
      "   User: Small Business Owner (Limited)\n",
      "\n",
      "3. persona_010_scenario_2\n",
      "   Task: Recover from error state related to: Confusing terminology\n",
      "   Focus: Error handling\n",
      "   User: Creative Professional (Limited)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PRIORITIZING TEST SCENARIOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Analyzing and prioritizing scenarios...\")\n",
    "\n",
    "# Prioritize all scenarios\n",
    "prioritized_scenarios = prioritize_scenarios(all_scenarios)\n",
    "\n",
    "# Count by priority\n",
    "priority_counts = {\n",
    "    'high': sum(1 for s in prioritized_scenarios if s['priority'] == 'high'),\n",
    "    'medium': sum(1 for s in prioritized_scenarios if s['priority'] == 'medium'),\n",
    "    'low': sum(1 for s in prioritized_scenarios if s['priority'] == 'low')\n",
    "}\n",
    "\n",
    "print(f\"\\n✓ Scenarios prioritized\")\n",
    "print(f\"\\nPriority Distribution:\")\n",
    "print(f\"  • High Priority: {priority_counts['high']} scenarios\")\n",
    "print(f\"  • Medium Priority: {priority_counts['medium']} scenarios\")\n",
    "print(f\"  • Low Priority: {priority_counts['low']} scenarios\")\n",
    "\n",
    "# Show top priority scenarios\n",
    "high_priority_scenarios = [s for s in prioritized_scenarios if s['priority'] == 'high']\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TOP 3 HIGH PRIORITY SCENARIOS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, scenario in enumerate(high_priority_scenarios[:3]):\n",
    "    print(f\"\\n{i+1}. {scenario['scenario_id']}\")\n",
    "    print(f\"   Task: {scenario['task_description']}\")\n",
    "    print(f\"   Focus: {scenario['focus_area']}\")\n",
    "    print(f\"   User: {scenario['user_context']['user_type']} ({scenario['user_context']['tech_proficiency']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d72e27-95cb-4cde-9de9-580fd1d79ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING TEST EXECUTION PLAN\n",
      "============================================================\n",
      "✓ Test plan created\n",
      "\n",
      "Test Plan Summary:\n",
      "  • Total scenarios: 150\n",
      "  • Personas tested: 30\n",
      "  • High priority: 20\n",
      "  • Medium priority: 47\n",
      "  • Low priority: 83\n",
      "  • Estimated time: 1125 minutes\n",
      "    (18.8 hours)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CREATING TEST EXECUTION PLAN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_plan = create_test_execution_plan(prioritized_scenarios, personas)\n",
    "\n",
    "print(\"✓ Test plan created\")\n",
    "print(f\"\\nTest Plan Summary:\")\n",
    "print(f\"  • Total scenarios: {test_plan['total_scenarios']}\")\n",
    "print(f\"  • Personas tested: {test_plan['total_personas']}\")\n",
    "print(f\"  • High priority: {test_plan['scenarios_by_priority']['high']}\")\n",
    "print(f\"  • Medium priority: {test_plan['scenarios_by_priority']['medium']}\")\n",
    "print(f\"  • Low priority: {test_plan['scenarios_by_priority']['low']}\")\n",
    "print(f\"  • Estimated time: {test_plan['estimated_duration']['minutes']} minutes\")\n",
    "print(f\"    ({test_plan['estimated_duration']['hours']} hours)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302c509c-271b-46d8-8871-824124d8c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING TEST EXECUTION PLAN\n",
      "============================================================\n",
      "✓ Test plan created\n",
      "\n",
      "Test Plan Summary:\n",
      "  • Total scenarios: 15\n",
      "  • Personas tested: 1\n",
      "  • Estimated time: 0 minutes\n",
      "  • (0.0 hours)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TEST EXECUTION PLAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_test_plan(scenarios, max_scenarios=15):\n",
    "    \"\"\"\n",
    "    Create structured test execution plan\n",
    "    \n",
    "    Args:\n",
    "        scenarios: List of prioritized scenarios\n",
    "        max_scenarios: Maximum scenarios to include in plan\n",
    "    \n",
    "    Returns:\n",
    "        Dict with test plan structure\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort by priority\n",
    "    priority_order = {'High': 0, 'Medium': 1, 'Low': 2, 'Unknown': 3}\n",
    "    sorted_scenarios = sorted(\n",
    "        scenarios,\n",
    "        key=lambda x: priority_order.get(x.get('priority', 'Unknown'), 3)\n",
    "    )[:max_scenarios]\n",
    "    \n",
    "    # Group by persona\n",
    "    by_persona = {}\n",
    "    for scenario in sorted_scenarios:\n",
    "        persona_name = scenario.get('persona_name', 'Unknown')\n",
    "        if persona_name not in by_persona:\n",
    "            by_persona[persona_name] = []\n",
    "        by_persona[persona_name].append(scenario)\n",
    "    \n",
    "    # Calculate estimated time\n",
    "    total_time = sum([\n",
    "        int(scenario.get('estimated_time', '10').split()[0])\n",
    "        for scenario in sorted_scenarios\n",
    "        if scenario.get('estimated_time')\n",
    "    ])\n",
    "    \n",
    "    test_plan = {\n",
    "        \"plan_name\": \"Tandem Test Execution Plan\",\n",
    "        \"created_date\": str(pd.Timestamp.now()),\n",
    "        \"total_scenarios\": len(sorted_scenarios),\n",
    "        \"estimated_time_minutes\": total_time,\n",
    "        \"personas_tested\": len(by_persona),\n",
    "        \"scenarios_by_persona\": by_persona,\n",
    "        \"execution_order\": [\n",
    "            {\n",
    "                \"sequence\": i,\n",
    "                \"scenario_id\": scenario.get('task_description', ''),\n",
    "                \"persona\": scenario.get('persona_name', ''),\n",
    "                \"priority\": scenario.get('priority', ''),\n",
    "                \"estimated_minutes\": scenario.get('estimated_time', 'Unknown')\n",
    "            }\n",
    "            for i, scenario in enumerate(sorted_scenarios, 1)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return test_plan\n",
    "\n",
    "test_plan = create_test_plan(prioritized_scenarios, max_scenarios=15)\n",
    "\n",
    "print(f\"✓ Test plan created\")\n",
    "print(f\"\\nTest Plan Summary:\")\n",
    "print(f\"  • Total scenarios: {test_plan['total_scenarios']}\")\n",
    "print(f\"  • Personas tested: {test_plan['personas_tested']}\")\n",
    "print(f\"  • Estimated time: {test_plan['estimated_time_minutes']} minutes\")\n",
    "print(f\"  • ({test_plan['estimated_time_minutes'] / 60:.1f} hours)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c1f67d-e0b7-4a7a-b283-a6ff6edfa106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING SCENARIOS AND TEST PLAN\n",
      "============================================================\n",
      "✓ Saved 150 scenarios to test_scenarios.json\n",
      "✓ Saved test plan to test_execution_plan.json\n",
      "✓ Saved readable summary to test_plan_summary.txt\n",
      "\n",
      "✓ Notebook 3 Complete!\n",
      "\n",
      "Generated:\n",
      "  • 150 test scenarios\n",
      "  • Test plan for 15 priority scenarios\n",
      "  • Covers 1 different personas\n",
      "\n",
      "Next: Open Notebook 4 to run AI-powered usability testing\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING SCENARIOS AND TEST PLAN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save all scenarios\n",
    "scenarios_path = \"test_scenarios.json\"\n",
    "with open(scenarios_path, 'w') as f:\n",
    "    json.dump(prioritized_scenarios, f, indent=2)\n",
    "print(f\"✓ Saved {len(prioritized_scenarios)} scenarios to {scenarios_path}\")\n",
    "\n",
    "# Save test plan\n",
    "plan_path = \"test_execution_plan.json\"\n",
    "with open(plan_path, 'w') as f:\n",
    "    json.dump(test_plan, f, indent=2)\n",
    "print(f\"✓ Saved test plan to {plan_path}\")\n",
    "\n",
    "# Create human-readable summary\n",
    "summary_path = \"test_plan_summary.txt\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"TANDEM TEST EXECUTION PLAN\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Created: {test_plan['created_date']}\\n\")\n",
    "    f.write(f\"Total Scenarios: {test_plan['total_scenarios']}\\n\")\n",
    "    f.write(f\"Personas: {test_plan['personas_tested']}\\n\")\n",
    "    f.write(f\"Estimated Time: {test_plan['estimated_time_minutes']} minutes\\n\\n\")\n",
    "    \n",
    "    f.write(\"EXECUTION ORDER:\\n\")\n",
    "    f.write(\"-\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    for item in test_plan['execution_order']:\n",
    "        f.write(f\"{item['sequence']}. {item['scenario_id']}\\n\")\n",
    "        f.write(f\"   Persona: {item['persona']} | Priority: {item['priority']} | Time: {item['estimated_minutes']}\\n\\n\")\n",
    "\n",
    "print(f\"✓ Saved readable summary to {summary_path}\")\n",
    "\n",
    "print(f\"\\n✓ Notebook 3 Complete!\")\n",
    "print(f\"\\nGenerated:\")\n",
    "print(f\"  • {len(prioritized_scenarios)} test scenarios\")\n",
    "print(f\"  • Test plan for {test_plan['total_scenarios']} priority scenarios\")\n",
    "print(f\"  • Covers {test_plan['personas_tested']} different personas\")\n",
    "print(f\"\\nNext: Open Notebook 4 to run AI-powered usability testing\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8644fa0-38e6-4875-b7b7-105de70e27ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
