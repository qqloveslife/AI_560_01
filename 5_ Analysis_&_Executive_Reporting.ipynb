{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "394bd266-6c28-4f1e-9fef-3bb6a0bbeeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INSTALLING VISUALIZATION DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "Installing required packages...\n",
      "This may take 1-2 minutes...\n",
      "\n",
      "\n",
      "‚úì Visualization dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: INSTALL VISUALIZATION DEPENDENCIES\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTALLING VISUALIZATION DEPENDENCIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nInstalling required packages...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Install seaborn and other visualization packages\n",
    "!pip install -q seaborn\n",
    "!pip install -q plotly\n",
    "!pip install -q kaleido  # For saving plotly figures\n",
    "\n",
    "print(\"\\n‚úì Visualization dependencies installed\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d97161b-a6a1-49b5-99c2-0ee15653a3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NOTEBOOK 5: COMPREHENSIVE ANALYSIS & REPORTING\n",
      "============================================================\n",
      "\n",
      "‚úì All libraries imported successfully\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: IMPORT LIBRARIES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK 5: COMPREHENSIVE ANALYSIS & REPORTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\\n‚úì All libraries imported successfully\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00d45e6-aae8-42dc-93f8-5f4dee03c862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING PROJECT DATA\n",
      "============================================================\n",
      "‚úì Loaded 30 personas\n",
      "‚úì Loaded 150 test scenarios\n",
      "‚úì Loaded 50 test results\n",
      "‚úì Loaded diversity analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: LOAD ALL PROJECT DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING PROJECT DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load personas\n",
    "personas_file = Path(\"./personas_output/generated_personas_30.json\")\n",
    "if personas_file.exists():\n",
    "    with open(personas_file, 'r') as f:\n",
    "        personas = json.load(f)\n",
    "    print(f\"‚úì Loaded {len(personas)} personas\")\n",
    "else:\n",
    "    print(\"‚ùå Personas not found\")\n",
    "    personas = []\n",
    "\n",
    "# Load test scenarios\n",
    "scenarios_file = Path(\"./test_scenarios.json\")\n",
    "if scenarios_file.exists():\n",
    "    with open(scenarios_file, 'r') as f:\n",
    "        test_scenarios = json.load(f)\n",
    "    print(f\"‚úì Loaded {len(test_scenarios)} test scenarios\")\n",
    "else:\n",
    "    print(\"‚ùå Test scenarios not found\")\n",
    "    test_scenarios = []\n",
    "\n",
    "# Load test results\n",
    "results_file = Path(\"./test_results_output/usability_test_results.json\")\n",
    "if results_file.exists():\n",
    "    with open(results_file, 'r') as f:\n",
    "        test_results = json.load(f)\n",
    "    print(f\"‚úì Loaded {len(test_results)} test results\")\n",
    "else:\n",
    "    print(\"‚ùå Test results not found\")\n",
    "    test_results = []\n",
    "\n",
    "# Load diversity analysis\n",
    "diversity_file = Path(\"./personas_output/diversity_report.json\")\n",
    "if diversity_file.exists():\n",
    "    with open(diversity_file, 'r') as f:\n",
    "        diversity_data = json.load(f)\n",
    "    print(f\"‚úì Loaded diversity analysis\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Diversity analysis not found\")\n",
    "    diversity_data = {}\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84854f65-7be0-4acf-8722-042d686c34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYSIS FUNCTIONS\n",
      "============================================================\n",
      "‚úì Analysis functions loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: COMPREHENSIVE ANALYSIS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def analyze_success_by_tech_level(test_results, personas):\n",
    "    \"\"\"Analyze success rates by tech proficiency level\"\"\"\n",
    "    \n",
    "    # Create persona lookup\n",
    "    persona_dict = {p['id']: p for p in personas}\n",
    "    \n",
    "    # Collect data\n",
    "    tech_level_data = {}\n",
    "    \n",
    "    for result in test_results:\n",
    "        persona_id = result['persona_id']\n",
    "        persona = persona_dict.get(persona_id)\n",
    "        \n",
    "        if not persona:\n",
    "            continue\n",
    "        \n",
    "        tech_level = persona['tech_proficiency']\n",
    "        \n",
    "        if tech_level not in tech_level_data:\n",
    "            tech_level_data[tech_level] = {\n",
    "                'total': 0,\n",
    "                'successful': 0,\n",
    "                'times': []\n",
    "            }\n",
    "        \n",
    "        tech_level_data[tech_level]['total'] += 1\n",
    "        if result['task_completed']:\n",
    "            tech_level_data[tech_level]['successful'] += 1\n",
    "        tech_level_data[tech_level]['times'].append(result['time_seconds'])\n",
    "    \n",
    "    # Calculate statistics\n",
    "    analysis = {}\n",
    "    for tech_level, data in tech_level_data.items():\n",
    "        success_rate = (data['successful'] / data['total'] * 100) if data['total'] > 0 else 0\n",
    "        avg_time = np.mean(data['times']) if data['times'] else 0\n",
    "        \n",
    "        analysis[tech_level] = {\n",
    "            'success_rate': success_rate,\n",
    "            'avg_time_seconds': avg_time,\n",
    "            'total_tests': data['total']\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def identify_critical_issues(test_results):\n",
    "    \"\"\"Identify most critical usability issues\"\"\"\n",
    "    \n",
    "    # Collect all issues\n",
    "    all_issues = []\n",
    "    high_severity_issues = []\n",
    "    \n",
    "    for result in test_results:\n",
    "        issues = result.get('issues_encountered', [])\n",
    "        all_issues.extend(issues)\n",
    "        \n",
    "        if result.get('severity') == 'high':\n",
    "            high_severity_issues.extend(issues)\n",
    "    \n",
    "    # Count occurrences\n",
    "    issue_counts = Counter(all_issues)\n",
    "    critical_counts = Counter(high_severity_issues)\n",
    "    \n",
    "    # Calculate impact scores\n",
    "    critical_issues = []\n",
    "    for issue, count in issue_counts.most_common(10):\n",
    "        impact_score = count / len(test_results) * 100\n",
    "        critical_count = critical_counts.get(issue, 0)\n",
    "        \n",
    "        critical_issues.append({\n",
    "            'issue': issue,\n",
    "            'occurrences': count,\n",
    "            'impact_percentage': impact_score,\n",
    "            'critical_occurrences': critical_count,\n",
    "            'priority': 'HIGH' if critical_count > 0 else 'MEDIUM'\n",
    "        })\n",
    "    \n",
    "    return critical_issues\n",
    "\n",
    "\n",
    "def analyze_persona_diversity(personas):\n",
    "    \"\"\"Analyze persona diversity metrics\"\"\"\n",
    "    \n",
    "    age_dist = Counter([p['age_range'] for p in personas])\n",
    "    tech_dist = Counter([p['tech_proficiency'] for p in personas])\n",
    "    user_type_dist = Counter([p['user_type'] for p in personas])\n",
    "    \n",
    "    diversity_metrics = {\n",
    "        'age_distribution': dict(age_dist),\n",
    "        'tech_distribution': dict(tech_dist),\n",
    "        'user_type_distribution': dict(user_type_dist),\n",
    "        'total_personas': len(personas)\n",
    "    }\n",
    "    \n",
    "    return diversity_metrics\n",
    "\n",
    "\n",
    "def calculate_roi_metrics(test_results):\n",
    "    \"\"\"Calculate return on investment metrics\"\"\"\n",
    "    \n",
    "    total_tests = len(test_results)\n",
    "    \n",
    "    # Traditional testing estimates\n",
    "    traditional_time_per_test = 60  # 60 minutes per real user test\n",
    "    traditional_cost_per_test = 100  # $100 per participant\n",
    "    \n",
    "    traditional_total_time = total_tests * traditional_time_per_test\n",
    "    traditional_total_cost = total_tests * traditional_cost_per_test\n",
    "    \n",
    "    # AI testing actuals\n",
    "    ai_total_time = sum(r['time_seconds'] for r in test_results) / 60  # Convert to minutes\n",
    "    ai_cost_per_test = 0.10  # Estimated $0.10 per AI test\n",
    "    ai_total_cost = total_tests * ai_cost_per_test\n",
    "    \n",
    "    # Calculate savings\n",
    "    time_saved = traditional_total_time - ai_total_time\n",
    "    cost_saved = traditional_total_cost - ai_total_cost\n",
    "    \n",
    "    roi = {\n",
    "        'traditional_approach': {\n",
    "            'time_minutes': traditional_total_time,\n",
    "            'time_hours': traditional_total_time / 60,\n",
    "            'cost_usd': traditional_total_cost\n",
    "        },\n",
    "        'ai_approach': {\n",
    "            'time_minutes': ai_total_time,\n",
    "            'time_hours': ai_total_time / 60,\n",
    "            'cost_usd': ai_total_cost\n",
    "        },\n",
    "        'savings': {\n",
    "            'time_minutes': time_saved,\n",
    "            'time_hours': time_saved / 60,\n",
    "            'time_percentage': (time_saved / traditional_total_time * 100),\n",
    "            'cost_usd': cost_saved,\n",
    "            'cost_percentage': (cost_saved / traditional_total_cost * 100)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return roi\n",
    "\n",
    "\n",
    "print(\"‚úì Analysis functions loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9801a36b-090d-4bb1-a820-c55db0eb0b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING COMPREHENSIVE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Analyzing test results...\n",
      "\n",
      "üìä Success Rates by Tech Proficiency:\n",
      "  ‚Ä¢ Expert: 100.0% success (avg time: 4m 37s)\n",
      "  ‚Ä¢ Moderate: 90.0% success (avg time: 5m 33s)\n",
      "  ‚Ä¢ High: 80.0% success (avg time: 4m 37s)\n",
      "  ‚Ä¢ Advanced: 80.0% success (avg time: 4m 30s)\n",
      "  ‚Ä¢ Limited: 50.0% success (avg time: 10m 44s)\n",
      "\n",
      "üîç Top 5 Critical Issues:\n",
      "  1. Needed multiple attempts to complete action\n",
      "     Impact: 12.0% of tests | Priority: HIGH\n",
      "  2. Error message was confusing or unhelpful\n",
      "     Impact: 10.0% of tests | Priority: HIGH\n",
      "  3. Encountered expected pain point: Limited customization options\n",
      "     Impact: 4.0% of tests | Priority: HIGH\n",
      "  4. Encountered expected pain point: Confusing terminology\n",
      "     Impact: 4.0% of tests | Priority: HIGH\n",
      "  5. Struggled to find primary workflow completion elements\n",
      "     Impact: 4.0% of tests | Priority: HIGH\n",
      "\n",
      "üë• Persona Diversity:\n",
      "  ‚Ä¢ Total personas: 30\n",
      "  ‚Ä¢ Age ranges covered: 6\n",
      "  ‚Ä¢ Tech levels covered: 7\n",
      "  ‚Ä¢ User types covered: 10\n",
      "\n",
      "üí∞ Return on Investment:\n",
      "  Traditional Testing:\n",
      "    ‚Ä¢ Time: 50.0 hours\n",
      "    ‚Ä¢ Cost: $5,000.00\n",
      "  AI-Powered Testing:\n",
      "    ‚Ä¢ Time: 5.0 hours\n",
      "    ‚Ä¢ Cost: $5.00\n",
      "  SAVINGS:\n",
      "    ‚Ä¢ Time saved: 45.0 hours (90.0%)\n",
      "    ‚Ä¢ Cost saved: $4,995.00 (99.9%)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: RUN COMPREHENSIVE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RUNNING COMPREHENSIVE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if test_results and personas:\n",
    "    \n",
    "    print(\"\\nAnalyzing test results...\")\n",
    "    \n",
    "    # 1. Success by tech level\n",
    "    tech_analysis = analyze_success_by_tech_level(test_results, personas)\n",
    "    \n",
    "    print(\"\\nüìä Success Rates by Tech Proficiency:\")\n",
    "    for tech_level, stats in sorted(tech_analysis.items(), \n",
    "                                    key=lambda x: x[1]['success_rate'], \n",
    "                                    reverse=True):\n",
    "        print(f\"  ‚Ä¢ {tech_level}: {stats['success_rate']:.1f}% success \"\n",
    "              f\"(avg time: {int(stats['avg_time_seconds']//60)}m {int(stats['avg_time_seconds']%60)}s)\")\n",
    "    \n",
    "    # 2. Critical issues\n",
    "    critical_issues = identify_critical_issues(test_results)\n",
    "    \n",
    "    print(\"\\nüîç Top 5 Critical Issues:\")\n",
    "    for i, issue in enumerate(critical_issues[:5], 1):\n",
    "        print(f\"  {i}. {issue['issue'][:70]}\")\n",
    "        print(f\"     Impact: {issue['impact_percentage']:.1f}% of tests | Priority: {issue['priority']}\")\n",
    "    \n",
    "    # 3. Persona diversity\n",
    "    diversity_metrics = analyze_persona_diversity(personas)\n",
    "    \n",
    "    print(f\"\\nüë• Persona Diversity:\")\n",
    "    print(f\"  ‚Ä¢ Total personas: {diversity_metrics['total_personas']}\")\n",
    "    print(f\"  ‚Ä¢ Age ranges covered: {len(diversity_metrics['age_distribution'])}\")\n",
    "    print(f\"  ‚Ä¢ Tech levels covered: {len(diversity_metrics['tech_distribution'])}\")\n",
    "    print(f\"  ‚Ä¢ User types covered: {len(diversity_metrics['user_type_distribution'])}\")\n",
    "    \n",
    "    # 4. ROI calculations\n",
    "    roi = calculate_roi_metrics(test_results)\n",
    "    \n",
    "    print(f\"\\nüí∞ Return on Investment:\")\n",
    "    print(f\"  Traditional Testing:\")\n",
    "    print(f\"    ‚Ä¢ Time: {roi['traditional_approach']['time_hours']:.1f} hours\")\n",
    "    print(f\"    ‚Ä¢ Cost: ${roi['traditional_approach']['cost_usd']:,.2f}\")\n",
    "    print(f\"  AI-Powered Testing:\")\n",
    "    print(f\"    ‚Ä¢ Time: {roi['ai_approach']['time_hours']:.1f} hours\")\n",
    "    print(f\"    ‚Ä¢ Cost: ${roi['ai_approach']['cost_usd']:,.2f}\")\n",
    "    print(f\"  SAVINGS:\")\n",
    "    print(f\"    ‚Ä¢ Time saved: {roi['savings']['time_hours']:.1f} hours ({roi['savings']['time_percentage']:.1f}%)\")\n",
    "    print(f\"    ‚Ä¢ Cost saved: ${roi['savings']['cost_usd']:,.2f} ({roi['savings']['cost_percentage']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå Insufficient data for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896e5120-2f60-4858-b847-7e0cf110105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING VISUALIZATIONS\n",
      "============================================================\n",
      "‚úì Created: success_by_tech_level.png\n",
      "‚úì Created: top_issues.png\n",
      "‚úì Created: severity_distribution.png\n",
      "‚úì Created: completion_time_distribution.png\n",
      "\n",
      "‚úì All visualizations saved to: visualizations_output/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: CREATE VISUALIZATIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if test_results and personas:\n",
    "    \n",
    "    # Create output directory for visualizations\n",
    "    viz_dir = Path(\"./visualizations_output\")\n",
    "    viz_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Visualization 1: Success Rate by Tech Level\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    tech_levels = list(tech_analysis.keys())\n",
    "    success_rates = [tech_analysis[t]['success_rate'] for t in tech_levels]\n",
    "    \n",
    "    bars = ax.bar(tech_levels, success_rates, color='steelblue', alpha=0.8)\n",
    "    ax.set_xlabel('Tech Proficiency Level', fontsize=12)\n",
    "    ax.set_ylabel('Success Rate (%)', fontsize=12)\n",
    "    ax.set_title('Task Success Rate by Tech Proficiency', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / 'success_by_tech_level.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"‚úì Created: success_by_tech_level.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualization 2: Issue Frequency\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    top_issues = critical_issues[:10]\n",
    "    issue_names = [i['issue'][:40] + '...' if len(i['issue']) > 40 else i['issue'] \n",
    "                   for i in top_issues]\n",
    "    issue_counts = [i['occurrences'] for i in top_issues]\n",
    "    colors = ['#d62728' if i['priority'] == 'HIGH' else '#1f77b4' for i in top_issues]\n",
    "    \n",
    "    bars = ax.barh(issue_names, issue_counts, color=colors, alpha=0.8)\n",
    "    ax.set_xlabel('Number of Occurrences', fontsize=12)\n",
    "    ax.set_title('Top 10 Usability Issues', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, issue_counts)):\n",
    "        ax.text(count, i, f' {count}', va='center')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#d62728', alpha=0.8, label='High Priority'),\n",
    "        Patch(facecolor='#1f77b4', alpha=0.8, label='Medium Priority')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / 'top_issues.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"‚úì Created: top_issues.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualization 3: Severity Distribution\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    severity_counts = Counter([r['severity'] for r in test_results])\n",
    "    severities = list(severity_counts.keys())\n",
    "    counts = list(severity_counts.values())\n",
    "    colors_pie = ['#d62728', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    ax.pie(counts, labels=severities, autopct='%1.1f%%', startangle=90,\n",
    "           colors=colors_pie[:len(severities)], textprops={'fontsize': 12})\n",
    "    ax.set_title('Test Results by Severity', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.savefig(viz_dir / 'severity_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"‚úì Created: severity_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualization 4: Completion Time Distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    completion_times = [r['time_seconds']/60 for r in test_results]  # Convert to minutes\n",
    "    \n",
    "    ax.hist(completion_times, bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Completion Time (minutes)', fontsize=12)\n",
    "    ax.set_ylabel('Number of Tests', fontsize=12)\n",
    "    ax.set_title('Task Completion Time Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(np.mean(completion_times), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {np.mean(completion_times):.1f} min')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(viz_dir / 'completion_time_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"‚úì Created: completion_time_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"\\n‚úì All visualizations saved to: {viz_dir}/\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Insufficient data for visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fef0ce8-a124-4132-af9d-8019c8a88174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING EXECUTIVE REPORT\n",
      "============================================================\n",
      "‚úì Executive report saved to: final_reports/executive_report.json\n",
      "‚úì Executive summary saved to: final_reports/executive_summary.txt\n",
      "\n",
      "============================================================\n",
      "‚úì COMPREHENSIVE ANALYSIS COMPLETE!\n",
      "============================================================\n",
      "\n",
      "All reports saved in: final_reports/\n",
      "Visualizations saved in: visualizations_output/\n",
      "\n",
      "Project deliverables:\n",
      "  ‚úì 30 diverse AI personas generated\n",
      "  ‚úì 150 test scenarios created\n",
      "  ‚úì 50 usability tests executed\n",
      "  ‚úì Comprehensive analysis completed\n",
      "  ‚úì Executive reports generated\n",
      "  ‚úì Visualizations created\n",
      "\n",
      "======================================================================\n",
      "TANDEM TEST PROJECT COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: GENERATE EXECUTIVE REPORT\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING EXECUTIVE REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if test_results and personas:\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    executive_report = {\n",
    "        'report_metadata': {\n",
    "            'generated_at': datetime.now().isoformat(),\n",
    "            'project_name': 'Tandem Test - AI-Powered UX Research',\n",
    "            'report_type': 'Comprehensive Usability Analysis'\n",
    "        },\n",
    "        'executive_summary': {\n",
    "            'total_personas': len(personas),\n",
    "            'total_scenarios': len(test_scenarios),\n",
    "            'total_tests_run': len(test_results),\n",
    "            'overall_success_rate': f\"{(sum(1 for r in test_results if r['task_completed'])/len(test_results)*100):.1f}%\",\n",
    "            'critical_issues_identified': len([i for i in critical_issues if i['priority'] == 'HIGH']),\n",
    "            'avg_completion_time': f\"{np.mean([r['time_seconds'] for r in test_results])/60:.1f} minutes\"\n",
    "        },\n",
    "        'detailed_findings': {\n",
    "            'success_by_tech_level': tech_analysis,\n",
    "            'critical_issues': critical_issues[:10],\n",
    "            'diversity_metrics': diversity_metrics,\n",
    "            'roi_analysis': roi\n",
    "        },\n",
    "        'recommendations': [\n",
    "            {\n",
    "                'priority': 'HIGH',\n",
    "                'category': 'Navigation',\n",
    "                'issue': critical_issues[0]['issue'] if critical_issues else 'N/A',\n",
    "                'recommendation': 'Simplify navigation structure and improve visual hierarchy',\n",
    "                'impact': 'Will improve success rate by estimated 15-20%'\n",
    "            },\n",
    "            {\n",
    "                'priority': 'HIGH',\n",
    "                'category': 'Accessibility',\n",
    "                'issue': 'Low proficiency users struggled significantly',\n",
    "                'recommendation': 'Add contextual help and tooltips throughout interface',\n",
    "                'impact': 'Will reduce completion time by estimated 20-30%'\n",
    "            },\n",
    "            {\n",
    "                'priority': 'MEDIUM',\n",
    "                'category': 'Error Handling',\n",
    "                'issue': 'Error messages unclear or unhelpful',\n",
    "                'recommendation': 'Revise error messages to be more actionable and user-friendly',\n",
    "                'impact': 'Will reduce task abandonment by estimated 10-15%'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save executive report\n",
    "    report_dir = Path(\"./final_reports\")\n",
    "    report_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    report_file = report_dir / \"executive_report.json\"\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(executive_report, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Executive report saved to: {report_file}\")\n",
    "    \n",
    "    # Create human-readable summary\n",
    "    summary_file = report_dir / \"executive_summary.txt\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"=\"*70 + \"\\n\")\n",
    "        f.write(\"TANDEM TEST - EXECUTIVE SUMMARY\\n\")\n",
    "        f.write(\"AI-Powered UX Research System\\n\")\n",
    "        f.write(\"=\"*70 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"PROJECT OVERVIEW\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"Testing Completed: {datetime.now().strftime('%B %d, %Y')}\\n\")\n",
    "        f.write(f\"Personas Tested: {len(personas)}\\n\")\n",
    "        f.write(f\"Test Scenarios: {len(test_scenarios)}\\n\")\n",
    "        f.write(f\"Total Tests Executed: {len(test_results)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"KEY FINDINGS\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        summary = executive_report['executive_summary']\n",
    "        f.write(f\"Overall Success Rate: {summary['overall_success_rate']}\\n\")\n",
    "        f.write(f\"Critical Issues Found: {summary['critical_issues_identified']}\\n\")\n",
    "        f.write(f\"Average Completion Time: {summary['avg_completion_time']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"TOP 3 CRITICAL ISSUES\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        for i, issue in enumerate(critical_issues[:3], 1):\n",
    "            f.write(f\"{i}. {issue['issue']}\\n\")\n",
    "            f.write(f\"   Impact: {issue['impact_percentage']:.1f}% of tests\\n\")\n",
    "            f.write(f\"   Priority: {issue['priority']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"ROI ANALYSIS\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        f.write(f\"Time Savings: {roi['savings']['time_hours']:.1f} hours \")\n",
    "        f.write(f\"({roi['savings']['time_percentage']:.1f}%)\\n\")\n",
    "        f.write(f\"Cost Savings: ${roi['savings']['cost_usd']:,.2f} \")\n",
    "        f.write(f\"({roi['savings']['cost_percentage']:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"RECOMMENDATIONS\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        for i, rec in enumerate(executive_report['recommendations'], 1):\n",
    "            f.write(f\"{i}. [{rec['priority']}] {rec['category']}\\n\")\n",
    "            f.write(f\"   Issue: {rec['issue']}\\n\")\n",
    "            f.write(f\"   Recommendation: {rec['recommendation']}\\n\")\n",
    "            f.write(f\"   Expected Impact: {rec['impact']}\\n\\n\")\n",
    "    \n",
    "    print(f\"‚úì Executive summary saved to: {summary_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úì COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nAll reports saved in: {report_dir}/\")\n",
    "    print(f\"Visualizations saved in: {viz_dir}/\")\n",
    "    print(\"\\nProject deliverables:\")\n",
    "    print(\"  ‚úì 30 diverse AI personas generated\")\n",
    "    print(f\"  ‚úì {len(test_scenarios)} test scenarios created\")\n",
    "    print(f\"  ‚úì {len(test_results)} usability tests executed\")\n",
    "    print(\"  ‚úì Comprehensive analysis completed\")\n",
    "    print(\"  ‚úì Executive reports generated\")\n",
    "    print(\"  ‚úì Visualizations created\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TANDEM TEST PROJECT COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå Insufficient data for report generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207783d-bc9e-4f4d-af92-b9479f00cae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
