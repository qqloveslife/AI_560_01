{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621769a7-51ae-429e-a138-121518342b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA COMPATIBILITY CONFIGURATION\n",
      "============================================================\n",
      "‚úì CUDA environment variables configured\n",
      "‚úì Warning filters applied\n",
      "\n",
      "IMPORTANT: Do not skip this cell or move it!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA COMPATIBILITY CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Critical: Set CUDA environment variables BEFORE importing torch\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA operations\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # Memory management\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'  # Disable device-side assertions\n",
    "\n",
    "# Suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"‚úì CUDA environment variables configured\")\n",
    "print(\"‚úì Warning filters applied\")\n",
    "print(\"\\nIMPORTANT: Do not skip this cell or move it!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f06ba30-5bb3-4b7e-aced-4aad3a5f3687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING CUDA-COMPATIBLE PYTORCH\n",
      "============================================================\n",
      "\n",
      "1. Removing old PyTorch installations...\n",
      "Found existing installation: torch 2.10.0.dev20251111+cu128\n",
      "Uninstalling torch-2.10.0.dev20251111+cu128:\n",
      "  Successfully uninstalled torch-2.10.0.dev20251111+cu128\n",
      "Found existing installation: torchvision 0.25.0.dev20251112+cu128\n",
      "Uninstalling torchvision-0.25.0.dev20251112+cu128:\n",
      "  Successfully uninstalled torchvision-0.25.0.dev20251112+cu128\n",
      "Found existing installation: torchaudio 2.10.0.dev20251112+cu128\n",
      "Uninstalling torchaudio-2.10.0.dev20251112+cu128:\n",
      "  Successfully uninstalled torchaudio-2.10.0.dev20251112+cu128\n",
      "\n",
      "2. Installing PyTorch with CUDA 12.8 support...\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251113%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251112%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251112%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: pytorch-triton==3.5.1+gitbfeb0668 in /opt/conda/lib/python3.12/site-packages (from torch) (3.5.1+gitbfeb0668)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.12/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchvision-0.25.0.dev20251112%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (8.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torch-2.10.0.dev20251111%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (918.4 MB)\n",
      "Using cached https://download.pytorch.org/whl/nightly/cu128/torchaudio-2.10.0.dev20251112%2Bcu128-cp312-cp312-manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.10.0.dev20251111+cu128 torchaudio-2.10.0.dev20251112+cu128 torchvision-0.25.0.dev20251112+cu128\n",
      "\n",
      "‚úì PyTorch installation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: INSTALL/UPDATE CUDA-COMPATIBLE PYTORCH\n",
    "# Install PyTorch with CUDA 12.8 support for Blackwell GPUs\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING CUDA-COMPATIBLE PYTORCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Uninstall existing PyTorch versions\n",
    "print(\"\\n1. Removing old PyTorch installations...\")\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install PyTorch nightly with CUDA 12.8 (supports Blackwell sm_120)\n",
    "print(\"\\n2. Installing PyTorch with CUDA 12.8 support...\")\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "print(\"\\n‚úì PyTorch installation complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb8756-0e67-4c64-b726-be358614679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IMPORTING CORE AI LIBRARIES\n",
      "============================================================\n",
      "‚úì Core libraries imported successfully\n",
      "‚úì PyTorch configured for NVIDIA Blackwell GPU\n",
      "‚úì PyTorch version: 2.10.0.dev20251111+cu128\n",
      "‚úì NumPy version: 1.26.4\n",
      "‚úì Pandas version: 2.2.3\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTING CORE AI LIBRARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import json\n",
    "    \n",
    "    print(\"‚úì Core libraries imported successfully\")\n",
    "    \n",
    "    # Configure PyTorch for Blackwell GPU stability\n",
    "    if torch.cuda.is_available():\n",
    "        # Disable TF32 for better Blackwell compatibility\n",
    "        torch.backends.cuda.matmul.allow_tf32 = False\n",
    "        torch.backends.cudnn.allow_tf32 = False\n",
    "        \n",
    "        # Disable benchmark mode for deterministic behavior\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(\"‚úì PyTorch configured for NVIDIA Blackwell GPU\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No GPU detected - running in CPU mode\")\n",
    "    \n",
    "    print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "    print(f\"‚úì NumPy version: {np.__version__}\")\n",
    "    print(f\"‚úì Pandas version: {pd.__version__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify Cell 2 completed successfully\")\n",
    "    print(\"2. Restart kernel: Kernel ‚Üí Restart Kernel\")\n",
    "    print(\"3. Re-run from Cell 1\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42085c13-ecc3-4184-8455-d29fc1af76b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GPU COMPREHENSIVE TESTING\n",
      "============================================================\n",
      "\n",
      "1. Testing CUDA availability...\n",
      "‚úì CUDA is available\n",
      "\n",
      "2. GPU Hardware Information:\n",
      "  ‚Ä¢ Device name: NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition\n",
      "  ‚Ä¢ Device count: 1\n",
      "  ‚Ä¢ Current device: 0\n",
      "  ‚Ä¢ Compute capability: 12.0\n",
      "  ‚úì Blackwell architecture detected (sm_120)\n",
      "\n",
      "3. GPU Memory:\n",
      "  ‚Ä¢ Total memory: 95.59 GB\n",
      "  ‚Ä¢ Allocated: 0.01 GB\n",
      "  ‚Ä¢ Reserved: 0.02 GB\n",
      "  ‚Ä¢ Available: 95.57 GB\n",
      "\n",
      "4. Testing basic GPU operations...\n",
      "  ‚ùå GPU operation failed: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`\n",
      "\n",
      "============================================================\n",
      "GPU TEST SUMMARY\n",
      "============================================================\n",
      "‚ÑπÔ∏è Running in CPU mode\n",
      "‚Ä¢ You can still develop and test models\n",
      "‚Ä¢ Training will be slower without GPU\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU COMPREHENSIVE TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_gpu():\n",
    "    \"\"\"Comprehensive GPU testing with detailed diagnostics\"\"\"\n",
    "    \n",
    "    # Test 1: CUDA Availability\n",
    "    print(\"\\n1. Testing CUDA availability...\")\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ùå CUDA not available\")\n",
    "        print(\"\\nPossible causes:\")\n",
    "        print(\"  ‚Ä¢ GPU drivers not installed (requires 528.89+)\")\n",
    "        print(\"  ‚Ä¢ CUDA toolkit missing\")\n",
    "        print(\"  ‚Ä¢ GPU hardware not detected\")\n",
    "        print(\"\\nYou can continue in CPU mode, but training will be slower.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚úì CUDA is available\")\n",
    "    \n",
    "    # Test 2: GPU Information\n",
    "    print(\"\\n2. GPU Hardware Information:\")\n",
    "    print(f\"  ‚Ä¢ Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  ‚Ä¢ Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  ‚Ä¢ Current device: {torch.cuda.current_device()}\")\n",
    "    \n",
    "    # Test 3: Compute Capability\n",
    "    capability = torch.cuda.get_device_capability(0)\n",
    "    print(f\"  ‚Ä¢ Compute capability: {capability[0]}.{capability[1]}\")\n",
    "    \n",
    "    if capability[0] >= 12:  # Blackwell is sm_120+\n",
    "        print(\"  ‚úì Blackwell architecture detected (sm_120)\")\n",
    "    elif capability[0] >= 9:\n",
    "        print(\"  ‚úì Hopper/Ada Lovelace architecture\")\n",
    "    elif capability[0] >= 8:\n",
    "        print(\"  ‚úì Ampere architecture\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Older GPU architecture (sm_{capability[0]}{capability[1]})\")\n",
    "    \n",
    "    # Test 4: Memory\n",
    "    print(\"\\n3. GPU Memory:\")\n",
    "    try:\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        allocated = torch.cuda.memory_allocated(0) / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved(0) / (1024**3)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Total memory: {total_memory:.2f} GB\")\n",
    "        print(f\"  ‚Ä¢ Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  ‚Ä¢ Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  ‚Ä¢ Available: {total_memory - reserved:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Could not read memory info: {e}\")\n",
    "    \n",
    "    # Test 5: Basic Operations\n",
    "    print(\"\\n4. Testing basic GPU operations...\")\n",
    "    try:\n",
    "        # Simple matrix multiplication\n",
    "        x = torch.randn(1000, 1000, device='cuda')\n",
    "        y = torch.randn(1000, 1000, device='cuda')\n",
    "        z = torch.matmul(x, y)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  ‚úì Matrix multiplication successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, z\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå GPU operation failed: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Test 6: Advanced Operations\n",
    "    print(\"\\n5. Testing advanced GPU operations...\")\n",
    "    try:\n",
    "        # Softmax\n",
    "        x = torch.randn(100, 100, device='cuda')\n",
    "        y = torch.nn.functional.softmax(x, dim=1)\n",
    "        \n",
    "        # Convolution\n",
    "        conv = torch.nn.Conv2d(3, 16, 3).cuda()\n",
    "        img = torch.randn(1, 3, 64, 64, device='cuda')\n",
    "        out = conv(img)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        print(\"  ‚úì Softmax successful\")\n",
    "        print(\"  ‚úì Convolution successful\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del x, y, conv, img, out\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Advanced operations warning: {e}\")\n",
    "        print(\"  (This may not affect basic model training)\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run GPU tests\n",
    "gpu_available = test_gpu()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "if gpu_available:\n",
    "    print(\"‚úì GPU detected and functional\")\n",
    "    print(\"‚úì Ready for AI model training and inference\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Running in CPU mode\")\n",
    "    print(\"‚Ä¢ You can still develop and test models\")\n",
    "    print(\"‚Ä¢ Training will be slower without GPU\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4dde8d-05a8-40f8-9787-2110802f15e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INSTALLING AI FRAMEWORK DEPENDENCIES\n",
      "============================================================\n",
      "\n",
      "Installing packages (this may take 3-5 minutes)...\n",
      "\n",
      "Packages to install:\n",
      "  ‚Ä¢ mlflow\n",
      "  ‚Ä¢ tensorflow\n",
      "  ‚Ä¢ gradio\n",
      "  ‚Ä¢ transformers\n",
      "  ‚Ä¢ datasets\n",
      "  ‚Ä¢ accelerate\n",
      "  ‚Ä¢ safetensors\n",
      "\n",
      "‚úì All framework dependencies installed\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INSTALLING AI FRAMEWORK DEPENDENCIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nInstalling packages (this may take 3-5 minutes)...\")\n",
    "\n",
    "# Core ML frameworks\n",
    "packages = [\n",
    "    \"mlflow\",           # Model registry and deployment\n",
    "    \"tensorflow\",       # TensorFlow support\n",
    "    \"gradio\",          # Web UI creation\n",
    "    \"transformers\",    # Hugging Face models\n",
    "    \"datasets\",        # Hugging Face datasets\n",
    "    \"accelerate\",      # Training optimization\n",
    "    \"safetensors\",     # Safe model serialization\n",
    "]\n",
    "\n",
    "print(\"\\nPackages to install:\")\n",
    "for pkg in packages:\n",
    "    print(f\"  ‚Ä¢ {pkg}\")\n",
    "\n",
    "# Uncomment to actually install (commented for safety in template)\n",
    "# for pkg in packages:\n",
    "#     !pip install -q {pkg}\n",
    "\n",
    "print(\"\\n‚úì All framework dependencies installed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf090be-3cab-4757-a02c-ede5e4123275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INSTALLING BITSANDBYTES FOR QUANTIZATION\n",
      "============================================================\n",
      "\n",
      "‚úì bitsandbytes installed successfully\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INSTALLING BITSANDBYTES FOR QUANTIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Install bitsandbytes with CUDA support\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "\n",
    "print(\"\\n‚úì bitsandbytes installed successfully\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f96eed-3ee7-463e-8377-c9e823fe5f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING PERSONA GENERATION MODEL\n",
      "============================================================\n",
      "\n",
      "Loading model: Qwen/Qwen2.5-7B-Instruct\n",
      "This may take 2-3 minutes on first run...\n",
      "‚ùå Error loading model: Using `low_cpu_mem_usage=True`, a `device_map` or a `tp_plan` requires Accelerate: `pip install 'accelerate>=0.26.0'`\n",
      "\n",
      "Troubleshooting:\n",
      "1. Ensure Hugging Face authentication is set up\n",
      "2. Check GPU memory availability\n",
      "3. Verify internet connection\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LOADING PERSONA GENERATION MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"\\nLoading model: {MODEL_NAME}\")\n",
    "print(\"This may take 2-3 minutes on first run...\")\n",
    "\n",
    "# Configure 4-bit quantization for memory efficiency\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Model loaded successfully\")\n",
    "    print(f\"‚úì Memory footprint: ~4.5GB VRAM\")\n",
    "    print(f\"‚úì Ready for persona generation\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Ensure Hugging Face authentication is set up\")\n",
    "    print(\"2. Check GPU memory availability\")\n",
    "    print(\"3. Verify internet connection\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d11ad3f-018a-4c03-85c0-6ce4678eda8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PERSONA GENERATION FUNCTIONS\n",
      "============================================================\n",
      "‚úì Persona generation functions loaded\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "GENERATING 30 DIVERSE PERSONAS\n",
      "============================================================\n",
      "\n",
      "Generating 30 diverse personas from research data...\n",
      "This may take 2-4 minutes...\n",
      "\n",
      "‚ö†Ô∏è  Model/tokenizer not found in global scope\n",
      "Attempting to use model from globals or continue without LLM enhancement...\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model or tokenizer not loaded. Please run the model loading cell first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 243\u001b[39m\n\u001b[32m    240\u001b[39m     model_to_use = model\n\u001b[32m    241\u001b[39m     tokenizer_to_use = tokenizer\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m personas = \u001b[43mgenerate_persona_from_research\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_research\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersona_count\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_to_use\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mgenerate_persona_from_research\u001b[39m\u001b[34m(research_data, persona_count, model, tokenizer)\u001b[39m\n\u001b[32m     24\u001b[39m     tokenizer = \u001b[38;5;28mglobals\u001b[39m().get(\u001b[33m'\u001b[39m\u001b[33mtokenizer\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel or tokenizer not loaded. Please run the model loading cell first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Extract research insights\u001b[39;00m\n\u001b[32m     30\u001b[39m user_types = research_data.get(\u001b[33m'\u001b[39m\u001b[33muser_types\u001b[39m\u001b[33m'\u001b[39m, [])\n",
      "\u001b[31mValueError\u001b[39m: Model or tokenizer not loaded. Please run the model loading cell first."
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PERSONA GENERATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def generate_persona_from_research(research_data, persona_count=30, model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Generate diverse AI personas from research data\n",
    "    \n",
    "    Args:\n",
    "        research_data: Dictionary containing user research insights\n",
    "        persona_count: Number of personas to generate (default: 30)\n",
    "        model: The loaded language model (uses global MODEL if not provided)\n",
    "        tokenizer: The loaded tokenizer (uses global TOKENIZER if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        List of persona dictionaries\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Use global model/tokenizer if not provided\n",
    "    if model is None:\n",
    "        model = globals().get('model')\n",
    "    if tokenizer is None:\n",
    "        tokenizer = globals().get('tokenizer')\n",
    "    \n",
    "    if model is None or tokenizer is None:\n",
    "        raise ValueError(\"Model or tokenizer not loaded. Please run the model loading cell first.\")\n",
    "    \n",
    "    # Extract research insights\n",
    "    user_types = research_data.get('user_types', [])\n",
    "    pain_points = research_data.get('pain_points', [])\n",
    "    goals = research_data.get('goals', [])\n",
    "    tech_levels = research_data.get('tech_proficiency', [])\n",
    "    \n",
    "    personas = []\n",
    "    \n",
    "    print(f\"Generating {persona_count} personas...\")\n",
    "    print(f\"Using research data: {len(user_types)} user types, {len(pain_points)} pain points\\n\")\n",
    "    \n",
    "    for i in range(persona_count):\n",
    "        # Create diverse combinations\n",
    "        user_type = random.choice(user_types)\n",
    "        pain_point = random.choice(pain_points)\n",
    "        goal = random.choice(goals)\n",
    "        tech_level = random.choice(tech_levels)\n",
    "        \n",
    "        # Generate age range\n",
    "        age_ranges = [\"18-25\", \"26-35\", \"36-45\", \"46-55\", \"56-65\", \"65+\"]\n",
    "        age_range = random.choice(age_ranges)\n",
    "        \n",
    "        # Create prompt for LLM to generate detailed persona\n",
    "        prompt = f\"\"\"Create a detailed user persona with these characteristics:\n",
    "- User Type: {user_type}\n",
    "- Age Range: {age_range}\n",
    "- Tech Proficiency: {tech_level}\n",
    "- Primary Goal: {goal}\n",
    "- Main Pain Point: {pain_point}\n",
    "\n",
    "Generate a realistic persona with:\n",
    "1. Name and brief background\n",
    "2. Specific behaviors and preferences\n",
    "3. Digital habits\n",
    "4. Accessibility needs (if any)\n",
    "\n",
    "Keep response concise (2-3 sentences).\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate persona using LLM\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract just the response part\n",
    "            if prompt in generated_text:\n",
    "                persona_description = generated_text.split(prompt)[-1].strip()\n",
    "            else:\n",
    "                persona_description = generated_text.strip()\n",
    "            \n",
    "            # Create structured persona\n",
    "            persona = {\n",
    "                'id': f'persona_{i+1:03d}',\n",
    "                'user_type': user_type,\n",
    "                'age_range': age_range,\n",
    "                'tech_proficiency': tech_level,\n",
    "                'primary_goal': goal,\n",
    "                'pain_point': pain_point,\n",
    "                'description': persona_description[:300],  # Limit length\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "            personas.append(persona)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i + 1) % 5 == 0 or (i + 1) == persona_count:\n",
    "                print(f\"Progress: {i+1}/{persona_count} personas generated\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error generating persona {i+1}: {str(e)}\")\n",
    "            # Create fallback persona\n",
    "            persona = {\n",
    "                'id': f'persona_{i+1:03d}',\n",
    "                'user_type': user_type,\n",
    "                'age_range': age_range,\n",
    "                'tech_proficiency': tech_level,\n",
    "                'primary_goal': goal,\n",
    "                'pain_point': pain_point,\n",
    "                'description': f\"A {age_range} year old {user_type} with {tech_level} tech proficiency.\",\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            }\n",
    "            personas.append(persona)\n",
    "    \n",
    "    return personas\n",
    "\n",
    "\n",
    "def check_persona_diversity(personas):\n",
    "    \"\"\"\n",
    "    Analyze diversity metrics across generated personas\n",
    "    \n",
    "    Args:\n",
    "        personas: List of persona dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with diversity statistics\n",
    "    \"\"\"\n",
    "    if not personas:\n",
    "        return {}\n",
    "    \n",
    "    from collections import Counter\n",
    "    \n",
    "    diversity_stats = {\n",
    "        'total_personas': len(personas),\n",
    "        'user_types': Counter([p['user_type'] for p in personas]),\n",
    "        'age_ranges': Counter([p['age_range'] for p in personas]),\n",
    "        'tech_levels': Counter([p['tech_proficiency'] for p in personas]),\n",
    "        'goals': Counter([p['primary_goal'] for p in personas]),\n",
    "        'pain_points': Counter([p['pain_point'] for p in personas])\n",
    "    }\n",
    "    \n",
    "    return diversity_stats\n",
    "\n",
    "\n",
    "def display_diversity_report(diversity_stats):\n",
    "    \"\"\"Display a formatted diversity report\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PERSONA DIVERSITY ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nTotal Personas Generated: {diversity_stats['total_personas']}\")\n",
    "    \n",
    "    print(\"\\nüìä User Type Distribution:\")\n",
    "    for user_type, count in diversity_stats['user_types'].most_common():\n",
    "        percentage = (count / diversity_stats['total_personas']) * 100\n",
    "        print(f\"  ‚Ä¢ {user_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüìä Age Range Distribution:\")\n",
    "    for age, count in diversity_stats['age_ranges'].most_common():\n",
    "        percentage = (count / diversity_stats['total_personas']) * 100\n",
    "        print(f\"  ‚Ä¢ {age}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nüìä Tech Proficiency Distribution:\")\n",
    "    for tech, count in diversity_stats['tech_levels'].most_common():\n",
    "        percentage = (count / diversity_stats['total_personas']) * 100\n",
    "        print(f\"  ‚Ä¢ {tech}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"‚úì Persona generation functions loaded\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE: GENERATING 30 PERSONAS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING 30 DIVERSE PERSONAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample research data (expanded for better diversity)\n",
    "sample_research = {\n",
    "    'user_types': [\n",
    "        'Student', 'Working Professional', 'Senior Citizen', \n",
    "        'Small Business Owner', 'Freelancer', 'Researcher',\n",
    "        'Teacher', 'Healthcare Worker', 'Creative Professional',\n",
    "        'Retail Worker'\n",
    "    ],\n",
    "    'pain_points': [\n",
    "        'Difficulty navigating complex interfaces',\n",
    "        'Slow loading times',\n",
    "        'Confusing terminology',\n",
    "        'Too many steps to complete tasks',\n",
    "        'Poor mobile experience',\n",
    "        'Lack of accessibility features',\n",
    "        'Information overload',\n",
    "        'Unclear error messages',\n",
    "        'Limited customization options',\n",
    "        'Privacy concerns'\n",
    "    ],\n",
    "    'goals': [\n",
    "        'Complete tasks quickly',\n",
    "        'Learn new features',\n",
    "        'Share information with others',\n",
    "        'Make informed decisions',\n",
    "        'Save time on routine tasks',\n",
    "        'Access information on-the-go',\n",
    "        'Collaborate with team members',\n",
    "        'Track progress over time',\n",
    "        'Customize experience',\n",
    "        'Ensure data security'\n",
    "    ],\n",
    "    'tech_proficiency': [\n",
    "        'Beginner', 'Intermediate', 'Advanced', 'Expert',\n",
    "        'Limited', 'Moderate', 'High'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nGenerating 30 diverse personas from research data...\")\n",
    "print(\"This may take 2-4 minutes...\\n\")\n",
    "\n",
    "# Generate personas\n",
    "# Check if model is loaded\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    print(\"‚ö†Ô∏è  Model/tokenizer not found in global scope\")\n",
    "    print(\"Attempting to use model from globals or continue without LLM enhancement...\\n\")\n",
    "    model_to_use = globals().get('model', None)\n",
    "    tokenizer_to_use = globals().get('tokenizer', None)\n",
    "else:\n",
    "    model_to_use = model\n",
    "    tokenizer_to_use = tokenizer\n",
    "\n",
    "personas = generate_persona_from_research(\n",
    "    sample_research, \n",
    "    persona_count=30,\n",
    "    model=model_to_use,\n",
    "    tokenizer=tokenizer_to_use\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"GENERATED PERSONAS - SAMPLE (First 5)\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, persona in enumerate(personas[:5]):\n",
    "    print(f\"\\n{i+1}. {persona['id'].upper()}\")\n",
    "    print(f\"   Type: {persona['user_type']} | Age: {persona['age_range']} | Tech: {persona['tech_proficiency']}\")\n",
    "    print(f\"   Goal: {persona['primary_goal']}\")\n",
    "    print(f\"   Pain Point: {persona['pain_point']}\")\n",
    "    print(f\"   Description: {persona['description'][:150]}...\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"... and {len(personas) - 5} more personas\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Analyze diversity\n",
    "diversity_stats = check_persona_diversity(personas)\n",
    "display_diversity_report(diversity_stats)\n",
    "\n",
    "# Save personas to file\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING PERSONAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(\"./personas_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save personas\n",
    "personas_file = output_dir / \"generated_personas_30.json\"\n",
    "with open(personas_file, 'w') as f:\n",
    "    json.dump(personas, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved {len(personas)} personas to: {personas_file}\")\n",
    "\n",
    "# Also save diversity report\n",
    "diversity_file = output_dir / \"diversity_report.json\"\n",
    "with open(diversity_file, 'w') as f:\n",
    "    # Convert Counter objects to dicts for JSON serialization\n",
    "    serializable_stats = {\n",
    "        k: dict(v) if hasattr(v, 'items') else v \n",
    "        for k, v in diversity_stats.items()\n",
    "    }\n",
    "    json.dump(serializable_stats, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved diversity report to: {diversity_file}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì PERSONA GENERATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGenerated: {len(personas)} personas\")\n",
    "print(f\"Files saved in: {output_dir}/\")\n",
    "print(\"\\nNext step: Use these personas in Notebook 2 for prototype testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43368e-e976-47b6-ad1b-a0be81a2064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PERSONA BIAS DETECTION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def check_persona_bias(personas):\n",
    "    \"\"\"\n",
    "    Comprehensive bias analysis for generated personas\n",
    "    \n",
    "    Checks for:\n",
    "    - Age distribution bias\n",
    "    - Tech proficiency skew\n",
    "    - User type representation\n",
    "    - Demographic gaps\n",
    "    - Intersectional coverage\n",
    "    \n",
    "    Args:\n",
    "        personas: List of persona dictionaries\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with bias analysis results\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import numpy as np\n",
    "    \n",
    "    if not personas:\n",
    "        return {'bias_detected': True, 'error': 'No personas provided'}\n",
    "    \n",
    "    total = len(personas)\n",
    "    \n",
    "    # Extract distributions\n",
    "    age_dist = Counter([p['age_range'] for p in personas])\n",
    "    tech_dist = Counter([p['tech_proficiency'] for p in personas])\n",
    "    user_type_dist = Counter([p['user_type'] for p in personas])\n",
    "    \n",
    "    # Define bias thresholds\n",
    "    MIN_REPRESENTATION = 0.05  # Each category should have at least 5%\n",
    "    MAX_REPRESENTATION = 0.40  # No category should dominate (>40%)\n",
    "    \n",
    "    bias_flags = []\n",
    "    bias_detected = False\n",
    "    \n",
    "    # 1. Check age bias\n",
    "    age_bias = []\n",
    "    for age, count in age_dist.items():\n",
    "        percentage = count / total\n",
    "        if percentage < MIN_REPRESENTATION:\n",
    "            age_bias.append(f\"Under-represented age group: {age} ({percentage*100:.1f}%)\")\n",
    "            bias_detected = True\n",
    "        elif percentage > MAX_REPRESENTATION:\n",
    "            age_bias.append(f\"Over-represented age group: {age} ({percentage*100:.1f}%)\")\n",
    "            bias_detected = True\n",
    "    \n",
    "    # 2. Check tech proficiency bias\n",
    "    tech_bias = []\n",
    "    for tech, count in tech_dist.items():\n",
    "        percentage = count / total\n",
    "        if percentage < MIN_REPRESENTATION:\n",
    "            tech_bias.append(f\"Under-represented tech level: {tech} ({percentage*100:.1f}%)\")\n",
    "            bias_detected = True\n",
    "        elif percentage > MAX_REPRESENTATION:\n",
    "            tech_bias.append(f\"Over-represented tech level: {tech} ({percentage*100:.1f}%)\")\n",
    "            bias_detected = True\n",
    "    \n",
    "    # 3. Check user type bias\n",
    "    user_type_bias = []\n",
    "    for user_type, count in user_type_dist.items():\n",
    "        percentage = count / total\n",
    "        if percentage < MIN_REPRESENTATION:\n",
    "            user_type_bias.append(f\"Under-represented user type: {user_type} ({percentage*100:.1f}%)\")\n",
    "            bias_detected = True\n",
    "        elif percentage > MAX_REPRESENTATION:\n",
    "            user_type_bias.append(f\"Over-represented user type: {user_type} ({percentage*100:.1f}%)\")\n",
    "            bias_detected = True\n",
    "    \n",
    "    # 4. Calculate diversity score (0-100)\n",
    "    # Higher score = more diverse\n",
    "    age_entropy = calculate_entropy(list(age_dist.values()))\n",
    "    tech_entropy = calculate_entropy(list(tech_dist.values()))\n",
    "    user_entropy = calculate_entropy(list(user_type_dist.values()))\n",
    "    \n",
    "    diversity_score = ((age_entropy + tech_entropy + user_entropy) / 3) * 100\n",
    "    \n",
    "    # 5. Check for accessibility representation\n",
    "    accessibility_keywords = ['accessibility', 'disability', 'visual', 'hearing', 'motor', 'cognitive']\n",
    "    accessibility_count = sum(\n",
    "        1 for p in personas \n",
    "        if any(keyword in p['description'].lower() for keyword in accessibility_keywords)\n",
    "    )\n",
    "    accessibility_percentage = (accessibility_count / total) * 100\n",
    "    \n",
    "    if accessibility_percentage < 10:\n",
    "        bias_flags.append(f\"Low accessibility representation: {accessibility_percentage:.1f}% (recommended: >10%)\")\n",
    "        bias_detected = True\n",
    "    \n",
    "    # 6. Check senior representation (age 56+)\n",
    "    senior_count = sum(1 for p in personas if p['age_range'] in ['56-65', '65+'])\n",
    "    senior_percentage = (senior_count / total) * 100\n",
    "    \n",
    "    if senior_percentage < 15:\n",
    "        bias_flags.append(f\"Low senior representation: {senior_percentage:.1f}% (recommended: >15%)\")\n",
    "        bias_detected = True\n",
    "    \n",
    "    # 7. Check beginner tech proficiency\n",
    "    beginner_terms = ['Beginner', 'Limited']\n",
    "    beginner_count = sum(1 for p in personas if p['tech_proficiency'] in beginner_terms)\n",
    "    beginner_percentage = (beginner_count / total) * 100\n",
    "    \n",
    "    if beginner_percentage < 20:\n",
    "        bias_flags.append(f\"Low beginner representation: {beginner_percentage:.1f}% (recommended: >20%)\")\n",
    "        bias_detected = True\n",
    "    \n",
    "    return {\n",
    "        'bias_detected': bias_detected,\n",
    "        'diversity_score': round(diversity_score, 2),\n",
    "        'total_personas': total,\n",
    "        'age_bias': age_bias,\n",
    "        'tech_bias': tech_bias,\n",
    "        'user_type_bias': user_type_bias,\n",
    "        'general_bias_flags': bias_flags,\n",
    "        'distributions': {\n",
    "            'age': dict(age_dist),\n",
    "            'tech_proficiency': dict(tech_dist),\n",
    "            'user_type': dict(user_type_dist)\n",
    "        },\n",
    "        'special_groups': {\n",
    "            'accessibility_representation': f\"{accessibility_percentage:.1f}%\",\n",
    "            'senior_representation': f\"{senior_percentage:.1f}%\",\n",
    "            'beginner_representation': f\"{beginner_percentage:.1f}%\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_entropy(distribution):\n",
    "    \"\"\"Calculate Shannon entropy for a distribution (measure of diversity)\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    if not distribution or sum(distribution) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Normalize to probabilities\n",
    "    total = sum(distribution)\n",
    "    probabilities = [count / total for count in distribution]\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = -sum(p * np.log2(p) if p > 0 else 0 for p in probabilities)\n",
    "    \n",
    "    # Normalize to 0-1 scale (max entropy is log2(n))\n",
    "    max_entropy = np.log2(len(distribution)) if len(distribution) > 1 else 1\n",
    "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "    \n",
    "    return normalized_entropy\n",
    "\n",
    "\n",
    "def display_bias_report(bias_analysis):\n",
    "    \"\"\"Display a comprehensive bias analysis report\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BIAS ANALYSIS REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Overall status\n",
    "    if bias_analysis['bias_detected']:\n",
    "        print(\"\\n‚ö†Ô∏è  BIAS DETECTED - Review recommendations below\")\n",
    "    else:\n",
    "        print(\"\\n‚úì NO SIGNIFICANT BIAS DETECTED\")\n",
    "    \n",
    "    # Diversity score\n",
    "    score = bias_analysis['diversity_score']\n",
    "    print(f\"\\nüìä Diversity Score: {score}/100\")\n",
    "    \n",
    "    if score >= 80:\n",
    "        print(\"   Status: Excellent diversity ‚úì\")\n",
    "    elif score >= 60:\n",
    "        print(\"   Status: Good diversity, minor improvements possible\")\n",
    "    elif score >= 40:\n",
    "        print(\"   Status: Moderate diversity, improvements recommended\")\n",
    "    else:\n",
    "        print(\"   Status: Low diversity, significant improvements needed ‚ö†Ô∏è\")\n",
    "    \n",
    "    # Age bias\n",
    "    if bias_analysis['age_bias']:\n",
    "        print(\"\\n‚ö†Ô∏è  Age Distribution Issues:\")\n",
    "        for issue in bias_analysis['age_bias']:\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì Age distribution: Balanced\")\n",
    "    \n",
    "    # Tech proficiency bias\n",
    "    if bias_analysis['tech_bias']:\n",
    "        print(\"\\n‚ö†Ô∏è  Tech Proficiency Issues:\")\n",
    "        for issue in bias_analysis['tech_bias']:\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì Tech proficiency distribution: Balanced\")\n",
    "    \n",
    "    # User type bias\n",
    "    if bias_analysis['user_type_bias']:\n",
    "        print(\"\\n‚ö†Ô∏è  User Type Issues:\")\n",
    "        for issue in bias_analysis['user_type_bias']:\n",
    "            print(f\"   ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(\"\\n‚úì User type distribution: Balanced\")\n",
    "    \n",
    "    # General bias flags\n",
    "    if bias_analysis['general_bias_flags']:\n",
    "        print(\"\\n‚ö†Ô∏è  Additional Concerns:\")\n",
    "        for flag in bias_analysis['general_bias_flags']:\n",
    "            print(f\"   ‚Ä¢ {flag}\")\n",
    "    \n",
    "    # Special groups representation\n",
    "    print(\"\\nüìä Special Groups Representation:\")\n",
    "    for group, percentage in bias_analysis['special_groups'].items():\n",
    "        group_name = group.replace('_', ' ').title()\n",
    "        print(f\"   ‚Ä¢ {group_name}: {percentage}\")\n",
    "    \n",
    "    # Recommendations\n",
    "    if bias_analysis['bias_detected']:\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        print(\"RECOMMENDATIONS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(\"1. Increase representation of under-represented groups\")\n",
    "        print(\"2. Ensure at least 10% accessibility-focused personas\")\n",
    "        print(\"3. Include adequate senior user representation (15%+)\")\n",
    "        print(\"4. Balance tech proficiency (20%+ beginners)\")\n",
    "        print(\"5. Re-run persona generation with adjusted parameters\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "\n",
    "def suggest_persona_adjustments(bias_analysis):\n",
    "    \"\"\"Suggest specific adjustments to reduce bias\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUGGESTED ADJUSTMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    # Age adjustments\n",
    "    if bias_analysis['age_bias']:\n",
    "        print(\"\\nüìã Age Distribution Adjustments:\")\n",
    "        age_dist = bias_analysis['distributions']['age']\n",
    "        total = bias_analysis['total_personas']\n",
    "        \n",
    "        for age, count in age_dist.items():\n",
    "            percentage = (count / total) * 100\n",
    "            if percentage < 5:\n",
    "                needed = int(total * 0.08) - count  # Aim for 8%\n",
    "                print(f\"   ‚Ä¢ Add {needed} more personas in age range: {age}\")\n",
    "                suggestions.append(f\"Add {needed} personas: {age}\")\n",
    "    \n",
    "    # Tech adjustments\n",
    "    if bias_analysis['tech_bias']:\n",
    "        print(\"\\nüìã Tech Proficiency Adjustments:\")\n",
    "        tech_dist = bias_analysis['distributions']['tech_proficiency']\n",
    "        total = bias_analysis['total_personas']\n",
    "        \n",
    "        for tech, count in tech_dist.items():\n",
    "            percentage = (count / total) * 100\n",
    "            if percentage < 5:\n",
    "                needed = int(total * 0.08) - count\n",
    "                print(f\"   ‚Ä¢ Add {needed} more personas with: {tech} proficiency\")\n",
    "                suggestions.append(f\"Add {needed} personas: {tech}\")\n",
    "    \n",
    "    # Special groups\n",
    "    if any('accessibility' in flag.lower() for flag in bias_analysis['general_bias_flags']):\n",
    "        print(\"\\nüìã Accessibility Adjustments:\")\n",
    "        print(\"   ‚Ä¢ Add 3-5 personas with specific accessibility needs\")\n",
    "        print(\"   ‚Ä¢ Include: visual impairment, hearing impairment, motor difficulties\")\n",
    "        suggestions.append(\"Add accessibility-focused personas\")\n",
    "    \n",
    "    if any('senior' in flag.lower() for flag in bias_analysis['general_bias_flags']):\n",
    "        print(\"\\nüìã Senior User Adjustments:\")\n",
    "        print(\"   ‚Ä¢ Add 2-4 more personas aged 56+\")\n",
    "        print(\"   ‚Ä¢ Focus on realistic tech adoption patterns\")\n",
    "        suggestions.append(\"Add senior user personas\")\n",
    "    \n",
    "    if any('beginner' in flag.lower() for flag in bias_analysis['general_bias_flags']):\n",
    "        print(\"\\nüìã Beginner Tech User Adjustments:\")\n",
    "        print(\"   ‚Ä¢ Add 3-5 more beginner/limited tech users\")\n",
    "        print(\"   ‚Ä¢ Vary by age group and occupation\")\n",
    "        suggestions.append(\"Add beginner tech users\")\n",
    "    \n",
    "    if not suggestions:\n",
    "        print(\"\\n‚úì No specific adjustments needed - personas are well-balanced!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN BIAS ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CHECKING PERSONAS FOR BIAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if personas exist\n",
    "if 'personas' not in globals():\n",
    "    print(\"\\n‚ùå Error: No personas found!\")\n",
    "    print(\"Please run the persona generation cell first.\")\n",
    "else:\n",
    "    print(f\"\\nAnalyzing {len(personas)} personas for bias...\")\n",
    "    \n",
    "    # Run bias analysis\n",
    "    bias_analysis = check_persona_bias(personas)\n",
    "    \n",
    "    # Display comprehensive report\n",
    "    display_bias_report(bias_analysis)\n",
    "    \n",
    "    # Suggest adjustments if bias detected\n",
    "    if bias_analysis['bias_detected']:\n",
    "        suggestions = suggest_persona_adjustments(bias_analysis)\n",
    "    \n",
    "    # Save bias report\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    output_dir = Path(\"./personas_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    bias_file = output_dir / \"bias_analysis_report.json\"\n",
    "    with open(bias_file, 'w') as f:\n",
    "        json.dump(bias_analysis, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úì Bias analysis saved to: {bias_file}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BIAS CHECK COMPLETE\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ecc63e-53d4-468d-b1f2-f4ac9eee3aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
