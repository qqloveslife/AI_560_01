{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55a30af-7860-4c53-8706-29f80ddd2334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NOTEBOOK 4: AI-POWERED USABILITY TESTING\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NOTEBOOK 4: AI-POWERED USABILITY TESTING\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa7c7f9-c3fe-4a68-9ef1-e46472905fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: IMPORT DEPENDENCIES\n",
    "# ============================================================import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "print(\"‚úì Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5a5aee-4963-46ef-a1d5-f2cdd4c4eaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING TEST DATA\n",
      "============================================================\n",
      "‚úì Loaded 150 test scenarios\n",
      "‚úì Loaded 30 personas\n",
      "‚úì Loaded prototype with 1 screens\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: LOAD PERSONAS, SCENARIOS, AND PROTOTYPE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load test scenarios from Notebook 3\n",
    "scenarios_file = Path(\"./test_scenarios.json\")\n",
    "if scenarios_file.exists():\n",
    "    with open(scenarios_file, 'r') as f:\n",
    "        test_scenarios = json.load(f)\n",
    "    print(f\"‚úì Loaded {len(test_scenarios)} test scenarios\")\n",
    "else:\n",
    "    print(\"‚ùå Test scenarios not found. Run Notebook 3 first!\")\n",
    "    test_scenarios = []\n",
    "\n",
    "# FIX: Correct path to personas\n",
    "personas_file = Path(\"./personas_output/generated_personas_30.json\")\n",
    "if personas_file.exists():\n",
    "    with open(personas_file, 'r') as f:\n",
    "        personas = json.load(f)\n",
    "    print(f\"‚úì Loaded {len(personas)} personas\")\n",
    "else:\n",
    "    print(\"‚ùå Personas not found. Run Notebook 1 first!\")\n",
    "    print(f\"   Looking for: {personas_file.absolute()}\")\n",
    "    personas = []\n",
    "\n",
    "# Load prototype from Notebook 2\n",
    "prototype_file = Path(\"prototype_data.json\")\n",
    "if prototype_file.exists():\n",
    "    with open(prototype_file, 'r') as f:\n",
    "        prototype_data = json.load(f)\n",
    "    print(f\"‚úì Loaded prototype with {prototype_data.get('screen_count', 0)} screens\")\n",
    "else:\n",
    "    print(\"‚ùå Prototype not found. Run Notebook 2 first!\")\n",
    "    prototype_data = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f097d3dc-1795-4003-931e-f57522cfc7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING AI MODEL FOR TESTING\n",
      "============================================================\n",
      "\n",
      "üìå Using Mistral-7B model (from Notebook 1)\n",
      "This model is already tested and working in your environment\n",
      "\n",
      "‚úì Model already loaded in memory\n",
      "‚úì Using existing model and tokenizer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: LOAD AI MODEL FOR TESTING\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING AI MODEL FOR TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use the Mistral model that's already working from Notebook 1\n",
    "# Avoid Qwen2VL due to PyTorch version incompatibility\n",
    "\n",
    "print(\"\\nüìå Using Mistral-7B model (from Notebook 1)\")\n",
    "print(\"This model is already tested and working in your environment\")\n",
    "\n",
    "# Check if model is already loaded in global scope\n",
    "if 'model' in globals() and 'tokenizer' in globals():\n",
    "    print(\"\\n‚úì Model already loaded in memory\")\n",
    "    print(f\"‚úì Using existing model and tokenizer\")\n",
    "else:\n",
    "    print(\"\\nLoading Mistral-7B-Instruct-v0.3...\")\n",
    "    print(\"This may take 1-2 minutes...\\n\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "        \n",
    "        MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "        \n",
    "        # Configure quantization for memory efficiency\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"‚úì Model loaded successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading model: {e}\")\n",
    "        print(\"\\nContinuing with rule-based testing approach...\")\n",
    "        model = None\n",
    "        tokenizer = None\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb87f050-7520-4948-affe-80923a88721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AI TESTING SIMULATION FUNCTIONS\n",
      "============================================================\n",
      "‚úì Testing simulation functions loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: AI TESTING SIMULATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AI TESTING SIMULATION FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def simulate_user_interaction(persona, scenario, prototype, model=None, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Simulate a persona interacting with the prototype for a scenario\n",
    "    \n",
    "    Args:\n",
    "        persona: Persona dictionary\n",
    "        scenario: Test scenario dictionary\n",
    "        prototype: Prototype data\n",
    "        model: AI model for simulation (optional)\n",
    "        tokenizer: Tokenizer (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Test result dictionary\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Extract key information\n",
    "    persona_id = persona['id']\n",
    "    scenario_id = scenario['scenario_id']\n",
    "    task = scenario['task_description']\n",
    "    success_criteria = scenario['success_criteria']\n",
    "    \n",
    "    # User context\n",
    "    user_type = persona['user_type']\n",
    "    tech_level = persona['tech_proficiency']\n",
    "    pain_point = persona['pain_point']\n",
    "    \n",
    "    # Initialize result\n",
    "    result = {\n",
    "        'test_id': f\"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{random.randint(1000,9999)}\",\n",
    "        'persona_id': persona_id,\n",
    "        'scenario_id': scenario_id,\n",
    "        'task': task,\n",
    "        'user_context': {\n",
    "            'user_type': user_type,\n",
    "            'tech_proficiency': tech_level,\n",
    "            'pain_point': pain_point\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Simulate based on user characteristics\n",
    "    # Beginners have lower success rate\n",
    "    base_success_rate = 0.85\n",
    "    \n",
    "    if tech_level in ['Beginner', 'Limited']:\n",
    "        base_success_rate = 0.65\n",
    "    elif tech_level in ['Intermediate', 'Moderate']:\n",
    "        base_success_rate = 0.80\n",
    "    elif tech_level in ['Advanced', 'Expert', 'High']:\n",
    "        base_success_rate = 0.95\n",
    "    \n",
    "    # Adjust for scenario difficulty\n",
    "    focus_area = scenario.get('focus_area', '')\n",
    "    if focus_area in ['Error handling', 'Secondary workflows']:\n",
    "        base_success_rate -= 0.1\n",
    "    \n",
    "    # Simulate outcome\n",
    "    success_roll = random.random()\n",
    "    task_completed = success_roll < base_success_rate\n",
    "    \n",
    "    # Generate realistic issues based on user characteristics\n",
    "    issues = []\n",
    "    confusion_points = []\n",
    "    \n",
    "    if not task_completed or random.random() < 0.3:\n",
    "        # Generate issues\n",
    "        if tech_level in ['Beginner', 'Limited']:\n",
    "            issues.extend([\n",
    "                f\"Struggled to find {focus_area.lower()} elements\",\n",
    "                f\"Unclear about terminology related to {task[:30]}...\",\n",
    "                \"Needed multiple attempts to complete action\"\n",
    "            ])\n",
    "        \n",
    "        if pain_point in scenario.get('task_description', ''):\n",
    "            issues.append(f\"Encountered expected pain point: {pain_point}\")\n",
    "        \n",
    "        if focus_area == 'Error handling':\n",
    "            issues.append(\"Error message was confusing or unhelpful\")\n",
    "        \n",
    "        confusion_points.extend([\n",
    "            \"Navigation path not intuitive\",\n",
    "            \"Button labels unclear\",\n",
    "            \"Too many options presented at once\"\n",
    "        ])\n",
    "    \n",
    "    # Use LLM for enhanced analysis if available\n",
    "    llm_insights = None\n",
    "    if model and tokenizer:\n",
    "        try:\n",
    "            prompt = f\"\"\"A {user_type} with {tech_level} tech proficiency is trying to: {task}\n",
    "Their main concern is: {pain_point}\n",
    "\n",
    "Describe 2-3 specific usability issues they might encounter. Be concise.\"\"\"\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                padding=True\n",
    "            )\n",
    "            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    attention_mask=inputs['attention_mask']\n",
    "                )\n",
    "            \n",
    "            llm_insights = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if prompt in llm_insights:\n",
    "                llm_insights = llm_insights.split(prompt)[-1].strip()\n",
    "            llm_insights = llm_insights[:200]\n",
    "            \n",
    "        except Exception as e:\n",
    "            llm_insights = None\n",
    "    \n",
    "    # Calculate time taken (realistic estimates)\n",
    "    base_time = 300  # 5 minutes base\n",
    "    if tech_level in ['Beginner', 'Limited']:\n",
    "        time_multiplier = random.uniform(1.5, 2.5)\n",
    "    elif tech_level in ['Intermediate', 'Moderate']:\n",
    "        time_multiplier = random.uniform(1.0, 1.5)\n",
    "    else:\n",
    "        time_multiplier = random.uniform(0.7, 1.2)\n",
    "    \n",
    "    time_seconds = int(base_time * time_multiplier)\n",
    "    \n",
    "    # Compile result\n",
    "    result.update({\n",
    "        'task_completed': task_completed,\n",
    "        'success_rate': round(base_success_rate, 2),\n",
    "        'time_seconds': time_seconds,\n",
    "        'time_readable': f\"{time_seconds // 60}m {time_seconds % 60}s\",\n",
    "        'issues_encountered': random.sample(issues, min(len(issues), 3)) if issues else [],\n",
    "        'confusion_points': random.sample(confusion_points, min(len(confusion_points), 2)) if confusion_points else [],\n",
    "        'llm_insights': llm_insights,\n",
    "        'meets_success_criteria': task_completed and len(issues) <= 1,\n",
    "        'severity': 'low' if task_completed and len(issues) == 0 else ('medium' if task_completed else 'high')\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def run_batch_tests(scenarios, personas, prototype, model=None, tokenizer=None, max_tests=50):\n",
    "    \"\"\"\n",
    "    Run a batch of AI-powered usability tests\n",
    "    \n",
    "    Args:\n",
    "        scenarios: List of test scenarios\n",
    "        personas: List of personas\n",
    "        prototype: Prototype data\n",
    "        model: AI model\n",
    "        tokenizer: Tokenizer\n",
    "        max_tests: Maximum number of tests to run\n",
    "    \n",
    "    Returns:\n",
    "        List of test results\n",
    "    \"\"\"\n",
    "    print(f\"\\nRunning batch of {min(max_tests, len(scenarios))} tests...\")\n",
    "    print(\"This may take 3-5 minutes...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Select scenarios to test\n",
    "    test_scenarios = scenarios[:max_tests]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios):\n",
    "        # Find matching persona\n",
    "        persona = next(\n",
    "            (p for p in personas if p['id'] == scenario['persona_id']),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        if not persona:\n",
    "            continue\n",
    "        \n",
    "        # Run simulation\n",
    "        result = simulate_user_interaction(\n",
    "            persona, \n",
    "            scenario, \n",
    "            prototype,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 10 == 0 or (i + 1) == len(test_scenarios):\n",
    "            print(f\"Progress: {i+1}/{len(test_scenarios)} tests completed\")\n",
    "    \n",
    "    print(f\"\\n‚úì Completed {len(results)} tests\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úì Testing simulation functions loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "302eaad0-1e79-45de-8b37-443753ec8631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING AI-POWERED USABILITY TESTS\n",
      "============================================================\n",
      "\n",
      "Test Configuration:\n",
      "  ‚Ä¢ Total scenarios available: 150\n",
      "  ‚Ä¢ High priority scenarios: 20\n",
      "  ‚Ä¢ Personas available: 30\n",
      "  ‚Ä¢ Running first 50 tests for this demo\n",
      "\n",
      "\n",
      "Running batch of 50 tests...\n",
      "This may take 3-5 minutes...\n",
      "\n",
      "Progress: 10/50 tests completed\n",
      "Progress: 20/50 tests completed\n",
      "Progress: 30/50 tests completed\n",
      "Progress: 40/50 tests completed\n",
      "Progress: 50/50 tests completed\n",
      "\n",
      "‚úì Completed 50 tests\n",
      "\n",
      "============================================================\n",
      "TEST EXECUTION COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: RUN AI-POWERED TESTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RUNNING AI-POWERED USABILITY TESTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not test_scenarios or not personas:\n",
    "    print(\"\\n‚ùå Cannot run tests - missing scenarios or personas\")\n",
    "    print(\"Please run Notebooks 1 and 3 first\")\n",
    "else:\n",
    "    # Run tests on high-priority scenarios first\n",
    "    high_priority = [s for s in test_scenarios if s.get('priority') == 'high']\n",
    "    \n",
    "    print(f\"\\nTest Configuration:\")\n",
    "    print(f\"  ‚Ä¢ Total scenarios available: {len(test_scenarios)}\")\n",
    "    print(f\"  ‚Ä¢ High priority scenarios: {len(high_priority)}\")\n",
    "    print(f\"  ‚Ä¢ Personas available: {len(personas)}\")\n",
    "    print(f\"  ‚Ä¢ Running first 50 tests for this demo\\n\")\n",
    "    \n",
    "    # Run batch tests\n",
    "    test_results = run_batch_tests(\n",
    "        scenarios=test_scenarios,\n",
    "        personas=personas,\n",
    "        prototype=prototype_data,\n",
    "        model=globals().get('model'),\n",
    "        tokenizer=globals().get('tokenizer'),\n",
    "        max_tests=50\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TEST EXECUTION COMPLETE\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f55602f6-90f0-48dd-91bb-4c7ca051e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYZING TEST RESULTS\n",
      "============================================================\n",
      "\n",
      "üìä Overall Statistics:\n",
      "  ‚Ä¢ Total tests run: 50\n",
      "  ‚Ä¢ Successful: 40 (80.0%)\n",
      "  ‚Ä¢ Failed: 10 (20.0%)\n",
      "  ‚Ä¢ Average completion time: 6m 1s\n",
      "\n",
      "‚ö†Ô∏è  Severity Breakdown:\n",
      "  ‚Ä¢ High severity: 10\n",
      "  ‚Ä¢ Medium severity: 3\n",
      "  ‚Ä¢ Low severity: 37\n",
      "\n",
      "üîç Top 5 Most Common Issues:\n",
      "  ‚Ä¢ Needed multiple attempts to complete action... (6 tests, 12.0%)\n",
      "  ‚Ä¢ Error message was confusing or unhelpful... (5 tests, 10.0%)\n",
      "  ‚Ä¢ Encountered expected pain point: Limited customization optio... (2 tests, 4.0%)\n",
      "  ‚Ä¢ Encountered expected pain point: Confusing terminology... (2 tests, 4.0%)\n",
      "  ‚Ä¢ Struggled to find primary workflow completion elements... (2 tests, 4.0%)\n",
      "\n",
      "------------------------------------------------------------\n",
      "SAMPLE TEST RESULTS (First 3):\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Test ID: test_20251113_130334_5462\n",
      "   Persona: Creative Professional (Expert)\n",
      "   Task: Navigate to main feature and complete tasks quickly...\n",
      "   Completed: ‚úì Yes\n",
      "   Time: 5m 30s\n",
      "   Severity: LOW\n",
      "\n",
      "2. Test ID: test_20251113_130334_1816\n",
      "   Persona: Creative Professional (Expert)\n",
      "   Task: Recover from error state related to: Lack of accessibility f...\n",
      "   Completed: ‚úì Yes\n",
      "   Time: 4m 17s\n",
      "   Severity: LOW\n",
      "\n",
      "3. Test ID: test_20251113_130334_6264\n",
      "   Persona: Creative Professional (Expert)\n",
      "   Task: Find and use help/information resources...\n",
      "   Completed: ‚úì Yes\n",
      "   Time: 4m 2s\n",
      "   Severity: LOW\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 6: ANALYZE TEST RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYZING TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'test_results' in locals() and test_results:\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_tests = len(test_results)\n",
    "    successful = sum(1 for r in test_results if r['task_completed'])\n",
    "    failed = total_tests - successful\n",
    "    success_rate = (successful / total_tests) * 100\n",
    "    \n",
    "    # Count issues\n",
    "    all_issues = []\n",
    "    for r in test_results:\n",
    "        all_issues.extend(r.get('issues_encountered', []))\n",
    "    \n",
    "    issue_counts = Counter(all_issues)\n",
    "    \n",
    "    # Severity breakdown\n",
    "    severity_counts = Counter([r['severity'] for r in test_results])\n",
    "    \n",
    "    # Average time\n",
    "    avg_time = sum(r['time_seconds'] for r in test_results) / total_tests\n",
    "    \n",
    "    print(f\"\\nüìä Overall Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total tests run: {total_tests}\")\n",
    "    print(f\"  ‚Ä¢ Successful: {successful} ({success_rate:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Failed: {failed} ({100-success_rate:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Average completion time: {int(avg_time//60)}m {int(avg_time%60)}s\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Severity Breakdown:\")\n",
    "    print(f\"  ‚Ä¢ High severity: {severity_counts.get('high', 0)}\")\n",
    "    print(f\"  ‚Ä¢ Medium severity: {severity_counts.get('medium', 0)}\")\n",
    "    print(f\"  ‚Ä¢ Low severity: {severity_counts.get('low', 0)}\")\n",
    "    \n",
    "    print(f\"\\nüîç Top 5 Most Common Issues:\")\n",
    "    for issue, count in issue_counts.most_common(5):\n",
    "        percentage = (count / total_tests) * 100\n",
    "        print(f\"  ‚Ä¢ {issue[:60]}... ({count} tests, {percentage:.1f}%)\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"SAMPLE TEST RESULTS (First 3):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, result in enumerate(test_results[:3]):\n",
    "        print(f\"\\n{i+1}. Test ID: {result['test_id']}\")\n",
    "        print(f\"   Persona: {result['user_context']['user_type']} ({result['user_context']['tech_proficiency']})\")\n",
    "        print(f\"   Task: {result['task'][:60]}...\")\n",
    "        print(f\"   Completed: {'‚úì Yes' if result['task_completed'] else '‚úó No'}\")\n",
    "        print(f\"   Time: {result['time_readable']}\")\n",
    "        print(f\"   Severity: {result['severity'].upper()}\")\n",
    "        if result.get('issues_encountered'):\n",
    "            print(f\"   Issues: {', '.join(result['issues_encountered'][:2])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No test results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3bed27-a22a-4c8b-9bd7-79a057665e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING TEST RESULTS\n",
      "============================================================\n",
      "‚úì Saved 50 test results to: test_results_output/usability_test_results.json\n",
      "‚úì Saved summary report to: test_results_output/test_summary.json\n",
      "\n",
      "============================================================\n",
      "‚úì TESTING COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Results saved in: test_results_output/\n",
      "\n",
      "Next step: Proceed to Notebook 5 for comprehensive analysis\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 7: SAVE TEST RESULTS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'test_results' in locals() and test_results:\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = Path(\"./test_results_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save full results\n",
    "    results_file = output_dir / \"usability_test_results.json\"\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved {len(test_results)} test results to: {results_file}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary = {\n",
    "        'test_date': datetime.now().isoformat(),\n",
    "        'total_tests': len(test_results),\n",
    "        'success_rate': f\"{success_rate:.1f}%\",\n",
    "        'avg_completion_time_seconds': int(avg_time),\n",
    "        'severity_breakdown': dict(severity_counts),\n",
    "        'top_issues': [\n",
    "            {'issue': issue, 'count': count, 'percentage': f\"{(count/total_tests)*100:.1f}%\"}\n",
    "            for issue, count in issue_counts.most_common(10)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    summary_file = output_dir / \"test_summary.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Saved summary report to: {summary_file}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úì TESTING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nResults saved in: {output_dir}/\")\n",
    "    print(\"\\nNext step: Proceed to Notebook 5 for comprehensive analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f876ba07-fc3e-4f6f-aa33-a1ff46178a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
